params_resp for rank 5 : [{'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.06913995742797852 to run on rank 5
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.14084458351135254 to run on rank 5
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.24377751350402832 to run on rank 5
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.3171546459197998 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.12087082862854 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.3099205493927002 to run on rank 5
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.8263859748840332 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.8958539962768555 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.946969747543335 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.042410135269165 to run on rank 5
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.114406108856201 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.1526522636413574 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.1951138973236084 to run on rank 5
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.2430002689361572 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.287827253341675 to run on rank 5
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.3420705795288086 to run on rank 5
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.5915632247924805 to run on rank 5
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.6439428329467773 to run on rank 5
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.8088481426239014 to run on rank 5
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.8539626598358154 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.0175771713256836 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.0786473751068115params_resp for rank 7 : [{'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.07464838027954102 to run on rank 7
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.1844408512115479 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.2460181713104248 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.3181555271148682 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.3775153160095215 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.4252314567565918 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.4837493896484375 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.5366628170013428 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.5865237712860107 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.9115066528320312 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.9563369750976562 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.0498459339141846 to run on rank 7
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.1050264835357666 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.1932239532470703 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.2396016120910645 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.3248672485351562 to run on rank 7
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.2584264278411865 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.3027865886688232 to run on rank 7
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.351226806640625 to run on rank 7
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.423604965209961 to run on rank 7
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.4846813678741455 to run on rank 7
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.5361146926879883 to run on rankparams_resp for rank 6 : [{'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.06828737258911133 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.13630986213684082 to run on rank 6
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.18611454963684082 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.23726153373718262 to run on rank 6
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.29000020027160645 to run on rank 6
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.5677535533905029 to run on rank 6
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.6669166088104248 to run on rank 6
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.72918701171875 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.8466627597808838 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.9189391136169434 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.9628183841705322 to run on rank 6
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.007455825805664 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.091822862625122 to run on rank 6
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.0869715213775635 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.13010573387146 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.188305616378784 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.089531183242798 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.181905269622803 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.231231212615967 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.351804256439209 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.407080888748169 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.449252605438232 to run on rankparams_resp for rank 4 : [{'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.06964421272277832 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.13379788398742676 to run on rank 4
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.2868168354034424 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.524423360824585 to run on rank 4
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.5938427448272705 to run on rank 4
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.6422724723815918 to run on rank 4
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.8130042552947998 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.2184083461761475 to run on rank 4
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.4184494018554688 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.4953529834747314 to run on rank 4
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.549588680267334 to run on rank 4
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.5981590747833252 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.6749248504638672 to run on rank 4
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.23213267326355 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.8824524879455566 to run on rank 4
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.934779405593872 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.892769813537598 to run on rank 4
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.983988046646118 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 5.047008991241455 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 5.114849805831909 to run on rank 4
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 5.1660754680633545 to run on rank 4
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 5.2093963623046875 to run on rank 4
model with paramsparams_resp for rank 9 : [{'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 300}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.06962323188781738 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.2508115768432617 to run on rank 9
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.47358179092407227 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.5220787525177002 to run on rank 9
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.6348159313201904 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.6885294914245605 to run on rank 9
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.7341310977935791 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.7909269332885742 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.281970739364624 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.338205337524414 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.9428684711456299 to run on rank 9
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.9954376220703125 to run on rank 9
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.0378458499908447 to run on rank 9
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.1668436527252197 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.237416982650757 to run on rank 9
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.300105571746826 to run on rank 9
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.351461887359619 to run on rank 9
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.4114534854888916 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 200} took 2.749605178833008 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300} took 3.1484222412109375 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 3.6132619380950928 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300} took 4.496155023574829 to run on rank 9
model with paramsparams_resp for rank 8 : [{'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.0914912223815918 to run on rank 8
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.2370238304138184 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.5544919967651367 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 2.6239051818847656 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.6997570991516113 to run on rank 8
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.707505941390991 to run on rank 8
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.7670164108276367 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.813227415084839 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.8625378608703613 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.909860610961914 to run on rank 8
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.961992025375366 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.010318279266357 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.067528963088989 to run on rank 8
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.111785650253296 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.246874094009399 to run on rank 8
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.295129299163818 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.347676753997803 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.396842956542969 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.462138891220093 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 5.948273658752441 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 6.018317461013794 to run on rank 8
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 6.063250780105591 to run on rankparams_resp for rank 14 : [{'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}]
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300} took 1.4116792678833008 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500} took 1.731299877166748 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 1.7895095348358154 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 2.0403974056243896 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 2.641371250152588 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 2.874213218688965 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700} took 3.0888381004333496 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 3.5432965755462646 to run on rank 14
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200} took 3.9142918586730957 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700} took 5.165451526641846 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 5.433244228363037 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 700} took 6.88077712059021 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300} took 7.687791585922241 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 500} took 8.378462553024292 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 500} took 8.428232908248901 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 8.465741872787476 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 8.53079342842102 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 600, 'steps': 700} took 8.66435980796814 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 200} took 8.726980686187744 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 8.784407615661621 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 8.904669761657715 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 8.959891319274902 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}params_resp for rank 12 : [{'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}]
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300} took 0.7288894653320312 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700} took 2.074532985687256 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 3.078746795654297 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300} took 4.628642559051514 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200} took 5.021660566329956 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200} took 6.111605167388916 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 6.398064374923706 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 7.019519090652466 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 200} took 7.8844218254089355 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 200, 'steps': 500} took 7.925609827041626 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 7.96383261680603 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200} took 8.078173637390137 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 800, 'steps': 200} took 8.157091617584229 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 800, 'steps': 500} took 8.258551359176636 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 300} took 8.338072299957275 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 700} took 8.404778242111206 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 500} took 8.494535446166992 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 700} took 8.564570188522339 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 8.646997928619385 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700} took 8.727505445480347 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300} took 8.873016357421875 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500} took 9.474220514297485 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300}params_resp for rank 16 : [{'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200}]
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 0.06743144989013672 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 0.1067056655883789 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 500} took 0.16347408294677734 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 0.2139599323272705 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 300, 'steps': 500} took 0.2723712921142578 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 0.3414154052734375 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700} took 0.47769975662231445 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 0.53493332862854 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 0.6181831359863281 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 0.7258086204528809 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 500} took 0.8502490520477295 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 0.948585033416748 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 1.0119259357452393 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 1.5520517826080322 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500} took 3.0251364707946777 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 700} took 4.729232311248779 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 5.157552480697632 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300} took 5.88969874382019 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500} took 7.223378658294678 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 8.092820882797241 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300} took 9.437172412872314 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}params_resp for rank 10 : [{'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 700}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 200} took 0.1735382080078125 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 700} took 0.2695186138153076 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200} took 0.33692002296447754 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300} took 0.4092700481414795 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 200} took 0.5058901309967041 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 700} took 0.6729130744934082 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 500} took 0.8079116344451904 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 300} took 0.8735744953155518 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700} took 1.1513659954071045 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 300} took 1.2190051078796387 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300} took 2.8523714542388916 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200} took 3.855616569519043 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 5.330055475234985 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 5.4887778759002686 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200} took 5.552552223205566 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 6.04355263710022 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300} took 7.217042446136475 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200} took 7.600891828536987 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500} took 9.369140386581421 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300} took 10.899856090545654 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 11.196357727050781 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 11.687606573104858 to run on rankparams_resp for rank 13 : [{'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 800, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 200}]
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 0.5720207691192627 to run on rank 13
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 2.0199201107025146 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 2.066244125366211 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 300} took 2.2292072772979736 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 2.374635696411133 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 2.7312817573547363 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500} took 3.6753814220428467 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 500} took 4.854128837585449 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700} took 6.5143866539001465 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 7.474893808364868 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 8.479913711547852 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500} took 9.514643907546997 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300} took 10.322072982788086 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500} took 11.718912124633789 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300} took 13.076411962509155 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 13.843368768692017 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 800, 'steps': 700} took 13.940244197845459 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 13.97849726676941 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 300} took 14.040639638900757 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 800, 'steps': 200} took 14.113708734512329 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 300} took 14.162123441696167 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 14.210863828659058 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 700}params_resp for rank 11 : [{'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}]
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 200} took 0.4918248653411865 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 0.5472097396850586 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200} took 0.8051633834838867 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500} took 1.4604389667510986 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300} took 2.0315284729003906 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300} took 2.8029625415802 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700} took 4.536174058914185 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 5.489687919616699 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 5.85871958732605 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300} took 6.291780233383179 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 7.3068835735321045 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 300} took 7.961327314376831 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500} took 9.349378824234009 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200} took 10.236849308013916 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 10.86191177368164 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 10.935541868209839 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 300} took 10.986842632293701 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 500} took 11.038100004196167 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 11.079158782958984 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700} took 11.225306510925293 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 600, 'steps': 700} took 11.93617057800293 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300} took 12.752943754196167 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}total parameter combinations to consider: 492
params_resp for rank 0 : [{'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.07258820533752441 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.12651538848876953 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.26281261444091797 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.31551313400268555 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.067471981048584 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.697038888931274 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.758917808532715 to run on rank 0
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 6.304046392440796 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 6.352185249328613 to run on rank 0
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 7.402869462966919 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 7.520252227783203 to run on rank 0
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 7.69590163230896 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 7.792139053344727 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 8.08522367477417 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 8.135332107543945 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 9.281781196594238 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 13.809876203536987 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 13.989659309387207 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.168494462966919 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 15.224742650985718 to run on rank 0
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 15.2763671875 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 15.32956314086914 params_resp for rank 15 : [{'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 700}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}]
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700} took 0.4372718334197998 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500} took 0.9446656703948975 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 1.4988088607788086 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 700} took 1.5728352069854736 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300} took 2.263176441192627 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 3.104401111602783 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 3.474076271057129 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 3.8833601474761963 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 6.88355016708374 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 7.224578380584717 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 7.838744640350342 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300} took 8.415653467178345 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500} took 9.21912693977356 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300} took 9.270430564880371 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 300} took 9.5217866897583 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 11.114331007003784 to run on rank 15
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 500} took 12.488655805587769 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 12.53395676612854 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 200} took 12.621789693832397 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 12.902948379516602 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700} took 14.62502145767212 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300} took 15.042866706848145 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}params_resp for rank 2 : [{'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.07259488105773926 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.7502927780151367 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 0.7990450859069824 to run on rank 2
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.8857686519622803 to run on rank 2
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 0.9464466571807861 to run on rank 2
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 0.9982912540435791 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.0619797706604004 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.1953887939453125 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.261993169784546 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.3266286849975586 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.3743677139282227 to run on rank 2
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.4144973754882812 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.4645683765411377 to run on rank 2
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.5547730922698975 to run on rank 2
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.9849612712860107 to run on rank 2
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 12.653565168380737 to run on rank 2
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 12.756747961044312 to run on rank 2
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 12.817346811294556 to run on rank 2
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 12.897639036178589 to run on rank 2
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 13.676871061325073 to run on rank 2
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.673540830612183 to run on rank 2
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 16.071701288223267 to run on rank 2
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}params_resp for rank 3 : [{'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.309990406036377 to run on rank 3
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 1.4721145629882812 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 1.5282843112945557 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 1.5734293460845947 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 2.761275291442871 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 2.825787305831909 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 14.46856689453125 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 14.534841775894165 to run on rank 3
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 14.603391885757446 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 14.646869659423828 to run on rank 3
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 14.692500829696655 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 14.790912866592407 to run on rank 3
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 14.879245281219482 to run on rank 3
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.955339670181274 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 17.022982835769653 to run on rank 3
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 17.073801279067993 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 17.1787211894989 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 17.41558837890625 to run on rank 3
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 17.57798457145691 to run on rank 3
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 17.62416672706604 to run on rank 3
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 17.745083570480347 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 17.79084873199463 to run on rank 3
model with paramsparams_resp for rank 1 : [{'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}]
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 7.816535472869873 to run on rank 1
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 8.984004735946655 to run on rank 1
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 9.062345266342163 to run on rank 1
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 9.121129274368286 to run on rank 1
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 10.116629600524902 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 10.899957656860352 to run on rank 1
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 11.002604007720947 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 11.06266188621521 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 11.18900203704834 to run on rank 1
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 11.295427083969116 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.451523780822754 to run on rank 1
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 16.617533445358276 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 16.68112587928772 to run on rank 1
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 17.905140161514282 to run on rank 1
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 19.521893978118896 to run on rank 1
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 19.568896055221558 to run on rank 1
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 20.193398475646973 to run on rank 1
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 20.980849981307983 to run on rank 1
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 21.806934118270874 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 21.85893726348877 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 21.91223692893982 to run on rank 1
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 21.952948331832886 to run on rank 1
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 5.2801454067230225 to run on rank 4
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 5.32790207862854 to run on rank 4
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 5.381155967712402 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 5.434601545333862 to run on rank 4
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 5.497614145278931 to run on rank 4
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 5.557396173477173 to run on rank 4
 to run on rank 5
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.123737096786499 to run on rank 5
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.174700975418091 to run on rank 5
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 3.2178549766540527 to run on rank 5
model with params {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 3.2641372680664062 to run on rank 5
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.372168779373169 to run on rank 5
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 3.454355478286743 to run on rank 5
 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.495105743408203 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.546680450439453 to run on rank 6
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.591730117797852 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.634392499923706 to run on rank 6
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.6754255294799805 to run on rank 6
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.721741199493408 to run on rank 6
 {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 17.852059602737427 to run on rank 3
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 17.898666858673096 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 17.948864698410034 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 18.004340410232544 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 18.057430505752563 to run on rank 3
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 19.15238308906555 to run on rank 3
 7
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.638489723205566 to run on rank 7
model with params {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 4.6937336921691895 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.778187990188599 to run on rank 7
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 4.846707344055176 to run on rank 7
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.903108358383179 to run on rank 7
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 4.942235469818115 to run on rank 7
 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 6.194303512573242 to run on rank 8
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 6.242945909500122 to run on rank 8
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 6.294673442840576 to run on rank 8
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 6.369775295257568 to run on rank 8
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 6.4610772132873535 to run on rank 8
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 6.512456893920898 to run on rank 8
 {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 5.479845762252808 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300} took 6.953984498977661 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200} took 8.075201034545898 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 300, 'steps': 700} took 8.1280357837677 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 300} took 8.18209171295166 to run on rank 9
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 8.230653047561646 to run on rank 9
 took 21.99643063545227 to run on rank 1
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 22.04422688484192 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 23.197941303253174 to run on rank 1
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 23.29868221282959 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 23.78316378593445 to run on rank 1
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 24.04394841194153 to run on rank 1
 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300} took 11.998309135437012 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200} took 12.302302122116089 to run on rank 10
model with params {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 13.363982677459717 to run on rank 10
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 300} took 13.486703872680664 to run on rank 10
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 13.536077737808228 to run on rank 10
model with params {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300} took 13.631426572799683 to run on rank 10
 took 9.791864156723022 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 500} took 9.987878322601318 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200} took 10.56349492073059 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300} took 11.432011365890503 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 12.932666063308716 to run on rank 12
model with params {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 13.185682773590088 to run on rank 12
 took 15.047672271728516 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500} took 18.035274028778076 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500} took 21.762287855148315 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 21.869822025299072 to run on rank 11
model with params {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700} took 22.27701735496521 to run on rank 11
model with params {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300} took 22.5634183883667 to run on rank 11
params_resp for rank 17 : [{'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 700}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 300, 'steps': 200}]
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200} took 0.08616518974304199 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200} took 0.36153674125671387 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300} took 0.739849328994751 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500} took 1.0332245826721191 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500} took 1.5133161544799805 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 500} took 1.7330942153930664 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 2.326420545578003 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 500} took 4.544193506240845 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200} took 4.687164068222046 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200} took 5.433219909667969 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300} took 6.32710862159729 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500} took 7.751157283782959 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 700} took 7.948062419891357 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200} took 8.078808784484863 to run on rank 17
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200} took 9.199890613555908 to run on rank 17
model with params {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 9.248230218887329 to run on rank 17
 took 14.409081935882568 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300} took 14.61705231666565 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700} took 15.345344543457031 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 15.401429653167725 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 16.128384590148926 to run on rank 13
model with params {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 200} took 17.17513656616211 to run on rank 13
 took 9.11866545677185 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700} took 9.492463827133179 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300} took 9.571796894073486 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 9.812954425811768 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 500} took 10.272763967514038 to run on rank 14
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500} took 11.044580459594727 to run on rank 14
 took 15.875998973846436 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 16.183780193328857 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700} took 17.185258626937866 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 500} took 18.14520788192749 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 600, 'steps': 200} took 18.193750858306885 to run on rank 15
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200} took 18.329076290130615 to run on rank 15
 took 11.122575998306274 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200} took 11.872362613677979 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300} took 14.907639980316162 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200} took 15.55781888961792 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300} took 16.805829286575317 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200} took 16.88752293586731 to run on rank 16
model with params {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200} took 16.965089797973633 to run on rank 16
 took 16.146651029586792 to run on rank 2
model with params {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 16.191117763519287 to run on rank 2
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 16.43537926673889 to run on rank 2
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 16.476730585098267 to run on rank 2
model with params {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 16.544281482696533 to run on rank 2
model with params {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 28.299499034881592 to run on rank 2
to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.381101369857788 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 15.44421935081482 to run on rank 0
model with params {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 15.586533546447754 to run on rank 0
model with params {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400} took 15.86626672744751 to run on rank 0
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400} took 17.06308126449585 to run on rank 0
model with params {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400} took 18.55113911628723 to run on rank 0
max attacks occurred with params: {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}
highest pop occurred with: {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}
final results from all runs: [{'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 427, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 54, 'num_targ_repr': 19, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 429, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 73, 'num_indisc_conc': 76, 'num_targ_repr': 22, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 560, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 23, 'num_indisc_conc': 23, 'num_targ_repr': 81, 'num_indisc_repr': 85}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 53, 'num_indisc_conc': 39, 'num_targ_repr': 34, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2487, 'total_num_attacks': 2613, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9130434782608695, 0.9023255813953488, 0.9023255813953488, 0.8935185185185185], [0.9054726368159204, 0.9202127659574468, 0.8766519823788547, 0.8490566037735849], [0.9076923076923077, 0.933649289099526, 0.9302325581395349, 0.8701298701298701]], 'num_targ_conc': 71, 'num_indisc_conc': 70, 'num_targ_repr': 11, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 848, 'total_num_attacks': 2789, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6962025316455697, 0.7468354430379747, 0.7692307692307693, 0.7878787878787878], [0.7469879518072289, 0.6376811594202898, 0.7666666666666667, 0.8157894736842105], [0.765625, 0.7272727272727273, 0.691358024691358, 0.8059701492537313]], 'num_targ_conc': 77, 'num_indisc_conc': 68, 'num_targ_repr': 19, 'num_indisc_repr': 14}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 446, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 19, 'num_indisc_conc': 18, 'num_targ_repr': 79, 'num_indisc_repr': 74}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 917, 'total_num_attacks': 2369, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.45121951219512196, 0.6268656716417911, 0.6818181818181818, 0.5454545454545454], [0.5352112676056338, 0.6060606060606061, 0.6363636363636364, 0.5568181818181818], [0.5189873417721519, 0.4186046511627907, 0.4827586206896552, 0.532258064516129]], 'num_targ_conc': 69, 'num_indisc_conc': 75, 'num_targ_repr': 24, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 434, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 19, 'num_indisc_conc': 20, 'num_targ_repr': 80, 'num_indisc_repr': 84}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 399, 'total_num_attacks': 2113, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral', 'neutral'], ['anti-violence', 'anti-violence', 'neutral', 'neutral'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9333333333333333, 0.41935483870967744, 0.43243243243243246, 0.4857142857142857], [0.6666666666666666, 0.8666666666666667, 0.4878048780487805, 0.5833333333333334], [0.40625, 0.3939393939393939, 0.5151515151515151, 0.4482758620689655]], 'num_targ_conc': 31, 'num_indisc_conc': 35, 'num_targ_repr': 47, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 552, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 28, 'num_indisc_conc': 21, 'num_targ_repr': 72, 'num_indisc_repr': 83}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 682, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 20, 'num_targ_repr': 80, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 533, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 22, 'num_indisc_conc': 21, 'num_targ_repr': 70, 'num_indisc_repr': 89}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 8, 'total_num_attacks': 998, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'neutral', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 0.0, 0.0], [1.0, 0.0, 1.0, 1.0], [1.0, 0.0, 1.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 40, 'num_targ_repr': 78, 'num_indisc_repr': 89}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 427, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 81, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 389, 'total_num_attacks': 2373, 'dominant_sentiments': [['neutral', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.45454545454545453, 0.41379310344827586, 0.40625, 0.7419354838709677], [0.41025641025641024, 0.5161290322580645, 0.5294117647058824, 0.4411764705882353], [0.5151515151515151, 0.6, 0.5, 0.37142857142857144]], 'num_targ_conc': 42, 'num_indisc_conc': 38, 'num_targ_repr': 32, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 1443, 'total_num_attacks': 6856, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.41228070175438597, 0.45925925925925926, 0.3482142857142857, 0.4626865671641791], [0.37209302325581395, 0.41134751773049644, 0.3925233644859813, 0.46017699115044247], [0.41228070175438597, 0.4, 0.3853211009174312, 0.4153846153846154]], 'num_targ_conc': 66, 'num_indisc_conc': 78, 'num_targ_repr': 15, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 466, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 82, 'num_indisc_conc': 68, 'num_targ_repr': 22, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 602, 'total_num_attacks': 2381, 'dominant_sentiments': [['neutral', 'sympathetic', 'neutral', 'neutral'], ['sympathetic', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'sympathetic', 'sympathetic']], 'percent_dominant_sentiments': [[0.41379310344827586, 0.48936170212765956, 0.44680851063829785, 0.42857142857142855], [0.4166666666666667, 0.38, 0.3584905660377358, 0.39285714285714285], [0.48717948717948717, 0.35714285714285715, 0.37777777777777777, 0.3488372093023256]], 'num_targ_conc': 44, 'num_indisc_conc': 42, 'num_targ_repr': 39, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 436, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 10, 'num_indisc_conc': 15, 'num_targ_repr': 67, 'num_indisc_repr': 97}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 447, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 83, 'num_targ_repr': 35, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 442, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 21, 'num_targ_repr': 73, 'num_indisc_repr': 91}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2, 'total_num_attacks': 424, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0]], 'num_targ_conc': 67, 'num_indisc_conc': 72, 'num_targ_repr': 22, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 454, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 69, 'num_indisc_conc': 72, 'num_targ_repr': 19, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 606, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 15, 'num_targ_repr': 86, 'num_indisc_repr': 71}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 173, 'total_num_attacks': 739, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 0.5, 0.8333333333333334, 0.5], [0.7142857142857143, 0.5384615384615384, 0.8181818181818182, 0.5333333333333333], [0.7619047619047619, 0.7222222222222222, 0.6923076923076923, 0.6]], 'num_targ_conc': 66, 'num_indisc_conc': 57, 'num_targ_repr': 55, 'num_indisc_repr': 58}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 388, 'total_num_attacks': 2483, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.96875, 0.9375, 1.0, 1.0], [1.0, 0.9393939393939394, 1.0, 0.96875], [0.9393939393939394, 0.9117647058823529, 0.975609756097561, 0.967741935483871]], 'num_targ_conc': 60, 'num_indisc_conc': 63, 'num_targ_repr': 20, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 974, 'total_num_attacks': 2503, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8928571428571429, 0.9324324324324325, 0.9390243902439024, 0.9047619047619048], [0.8888888888888888, 0.8837209302325582, 0.8928571428571429, 0.948051948051948], [0.8842105263157894, 0.9358974358974359, 0.9014084507042254, 0.9054054054054054]], 'num_targ_conc': 52, 'num_indisc_conc': 90, 'num_targ_repr': 15, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 4862, 'total_num_attacks': 6043, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.3825, 0.38215102974828374, 0.36341463414634145, 0.3971631205673759], [0.4166666666666667, 0.4054794520547945, 0.38235294117647056, 0.4392764857881137], [0.3713592233009709, 0.3925, 0.3871794871794872, 0.38386308068459657]], 'num_targ_conc': 67, 'num_indisc_conc': 72, 'num_targ_repr': 20, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 478, 'total_num_attacks': 2226, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'sympathetic'], ['sympathetic', 'neutral', 'sympathetic', 'sympathetic'], ['anti-violence', 'neutral', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.3488372093023256, 0.3953488372093023, 0.5526315789473685, 0.47619047619047616], [0.42105263157894735, 0.41025641025641024, 0.4418604651162791, 0.5142857142857142], [0.5161290322580645, 0.5294117647058824, 0.75, 0.4857142857142857]], 'num_targ_conc': 67, 'num_indisc_conc': 69, 'num_targ_repr': 20, 'num_indisc_repr': 14}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 497, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 53, 'num_indisc_conc': 42, 'num_targ_repr': 53, 'num_indisc_repr': 46}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 439, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 29, 'num_targ_repr': 75, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 396, 'total_num_attacks': 2143, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'anti-violence', 'sympathetic'], ['anti-violence', 'sympathetic', 'sympathetic', 'sympathetic'], ['sympathetic', 'anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5454545454545454, 0.48484848484848486, 0.6129032258064516, 0.5], [0.41379310344827586, 0.5, 0.37142857142857144, 0.4838709677419355], [0.5277777777777778, 0.37142857142857144, 0.5806451612903226, 0.4117647058823529]], 'num_targ_conc': 39, 'num_indisc_conc': 40, 'num_targ_repr': 42, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 362, 'total_num_attacks': 1671, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4411764705882353, 0.6129032258064516, 0.8275862068965517, 0.7], [0.4838709677419355, 0.6764705882352942, 0.8846153846153846, 0.6764705882352942], [0.8095238095238095, 0.6774193548387096, 0.5483870967741935, 0.6451612903225806]], 'num_targ_conc': 15, 'num_indisc_conc': 15, 'num_targ_repr': 84, 'num_indisc_repr': 69}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 526, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 20, 'num_targ_repr': 77, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 462, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 18, 'num_targ_repr': 79, 'num_indisc_repr': 63}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 620, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 28, 'num_targ_repr': 47, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 528, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 14, 'num_indisc_conc': 19, 'num_targ_repr': 62, 'num_indisc_repr': 85}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 3151, 'total_num_attacks': 4139, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7706093189964157, 0.7490196078431373, 0.7509293680297398, 0.7243816254416962], [0.7736625514403292, 0.7520661157024794, 0.7326388888888888, 0.7166666666666667], [0.7192307692307692, 0.7282608695652174, 0.7378277153558053, 0.7222222222222222]], 'num_targ_conc': 61, 'num_indisc_conc': 67, 'num_targ_repr': 14, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 353, 'total_num_attacks': 2451, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.5714285714285714, 0.46875, 0.43333333333333335, 0.4074074074074074], [0.5161290322580645, 0.5333333333333333, 0.4642857142857143, 0.4482758620689655], [0.4, 0.42424242424242425, 0.5, 0.45454545454545453]], 'num_targ_conc': 42, 'num_indisc_conc': 38, 'num_targ_repr': 44, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 472, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 44, 'num_indisc_conc': 24, 'num_targ_repr': 40, 'num_indisc_repr': 54}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 405, 'total_num_attacks': 2268, 'dominant_sentiments': [['sympathetic', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'sympathetic', 'anti-violence'], ['sympathetic', 'neutral', 'sympathetic', 'sympathetic']], 'percent_dominant_sentiments': [[0.5483870967741935, 0.45454545454545453, 0.5476190476190477, 0.4838709677419355], [0.4594594594594595, 0.41935483870967744, 0.4358974358974359, 0.375], [0.5, 0.4838709677419355, 0.5172413793103449, 0.4864864864864865]], 'num_targ_conc': 35, 'num_indisc_conc': 33, 'num_targ_repr': 35, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 861, 'total_num_attacks': 2888, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4878048780487805, 0.589041095890411, 0.4594594594594595, 0.5], [0.5454545454545454, 0.4864864864864865, 0.4657534246575342, 0.6027397260273972], [0.527027027027027, 0.5833333333333334, 0.5285714285714286, 0.6]], 'num_targ_conc': 81, 'num_indisc_conc': 61, 'num_targ_repr': 14, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 424, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 20, 'num_targ_repr': 62, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 79, 'total_num_attacks': 1626, 'dominant_sentiments': [['neutral', 'sympathetic', 'sympathetic', 'neutral'], ['anti-violence', 'sympathetic', 'neutral', 'anti-violence'], ['sympathetic', 'neutral', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.6, 0.625, 0.875, 0.5], [0.5, 0.625, 0.4444444444444444, 0.5], [0.4444444444444444, 0.5, 0.4, 0.625]], 'num_targ_conc': 16, 'num_indisc_conc': 19, 'num_targ_repr': 83, 'num_indisc_repr': 65}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 38, 'total_num_attacks': 1895, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 1.0, 0.6666666666666666], [1.0, 0.8571428571428571, 1.0, 0.6], [1.0, 0.6666666666666666, 1.0, 1.0]], 'num_targ_conc': 45, 'num_indisc_conc': 33, 'num_targ_repr': 35, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 2081, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 19, 'num_indisc_conc': 19, 'num_targ_repr': 69, 'num_indisc_repr': 80}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 431, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 41, 'num_targ_repr': 44, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 441, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 30, 'num_targ_repr': 50, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 412, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 27, 'num_indisc_conc': 15, 'num_targ_repr': 83, 'num_indisc_repr': 66}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 28, 'num_indisc_conc': 13, 'num_targ_repr': 83, 'num_indisc_repr': 78}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 414, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 25, 'num_targ_repr': 71, 'num_indisc_repr': 82}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 369, 'total_num_attacks': 2473, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'neutral', 'neutral'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'sympathetic', 'anti-violence']], 'percent_dominant_sentiments': [[0.45161290322580644, 0.6774193548387096, 0.6206896551724138, 0.45454545454545453], [0.71875, 0.8846153846153846, 0.45454545454545453, 0.8928571428571429], [0.6666666666666666, 0.4, 0.5625, 0.8]], 'num_targ_conc': 38, 'num_indisc_conc': 42, 'num_targ_repr': 35, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 546, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 19, 'num_indisc_conc': 18, 'num_targ_repr': 83, 'num_indisc_repr': 77}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 1388, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 44, 'num_targ_repr': 78, 'num_indisc_repr': 64}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 611, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 11, 'num_targ_repr': 78, 'num_indisc_repr': 79}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 26, 'num_indisc_conc': 17, 'num_targ_repr': 65, 'num_indisc_repr': 76}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 362, 'total_num_attacks': 1202, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5757575757575758, 0.5757575757575758, 0.6551724137931034, 0.9], [0.5, 0.6666666666666666, 0.4444444444444444, 0.6129032258064516], [0.5, 0.5862068965517241, 0.6666666666666666, 0.5]], 'num_targ_conc': 18, 'num_indisc_conc': 11, 'num_targ_repr': 68, 'num_indisc_repr': 68}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 426, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 80, 'num_targ_repr': 29, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 523, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 42, 'num_targ_repr': 48, 'num_indisc_repr': 62}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 470, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 40, 'num_indisc_conc': 42, 'num_targ_repr': 40, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 442, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 13, 'num_targ_repr': 91, 'num_indisc_repr': 90}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 447, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 69, 'num_indisc_conc': 63, 'num_targ_repr': 41, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 13, 'total_num_attacks': 540, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0]], 'num_targ_conc': 83, 'num_indisc_conc': 76, 'num_targ_repr': 28, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 453, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 79, 'num_targ_repr': 17, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 456, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 12, 'num_targ_repr': 71, 'num_indisc_repr': 81}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 66, 'num_indisc_conc': 64, 'num_targ_repr': 23, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 417, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 46, 'num_targ_repr': 38, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 1, 'total_num_attacks': 429, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 70, 'num_targ_repr': 22, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 504, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 13, 'num_indisc_conc': 22, 'num_targ_repr': 83, 'num_indisc_repr': 90}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 846, 'total_num_attacks': 2566, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9367088607594937, 0.9375, 0.9508196721311475, 0.9518072289156626], [0.9344262295081968, 0.9594594594594594, 0.9491525423728814, 0.9733333333333334], [0.9428571428571428, 0.9142857142857143, 0.9444444444444444, 0.9295774647887324]], 'num_targ_conc': 83, 'num_indisc_conc': 70, 'num_targ_repr': 19, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 6676, 'total_num_attacks': 5156, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9351535836177475, 0.9357142857142857, 0.9332129963898917, 0.9210977701543739], [0.9191564147627417, 0.9232053422370617, 0.9142335766423357, 0.9381443298969072], [0.9245960502692998, 0.9327272727272727, 0.9220338983050848, 0.9372759856630825]], 'num_targ_conc': 75, 'num_indisc_conc': 78, 'num_targ_repr': 17, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 517, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 61, 'num_indisc_conc': 70, 'num_targ_repr': 25, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 466, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 44, 'num_indisc_conc': 34, 'num_targ_repr': 54, 'num_indisc_repr': 59}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 510, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 45, 'num_indisc_conc': 43, 'num_targ_repr': 46, 'num_indisc_repr': 60}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 121, 'total_num_attacks': 1942, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8666666666666667, 0.7692307692307693, 0.7142857142857143, 1.0], [0.8181818181818182, 1.0, 0.7857142857142857, 0.9090909090909091], [0.6666666666666666, 0.8, 0.8888888888888888, 1.0]], 'num_targ_conc': 21, 'num_indisc_conc': 15, 'num_targ_repr': 72, 'num_indisc_repr': 61}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 863, 'total_num_attacks': 3473, 'dominant_sentiments': [['neutral', 'anti-violence', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.42, 0.4125, 0.43103448275862066, 0.4375], [0.4411764705882353, 0.4868421052631579, 0.5416666666666666, 0.41935483870967744], [0.42045454545454547, 0.45569620253164556, 0.47058823529411764, 0.38271604938271603]], 'num_targ_conc': 66, 'num_indisc_conc': 61, 'num_targ_repr': 16, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 813, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 45, 'num_targ_repr': 55, 'num_indisc_repr': 80}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2, 'total_num_attacks': 494, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 68, 'num_indisc_conc': 63, 'num_targ_repr': 28, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 429, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 68, 'num_targ_repr': 19, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 10, 'total_num_attacks': 784, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0]], 'num_targ_conc': 69, 'num_indisc_conc': 70, 'num_targ_repr': 63, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 411, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 19, 'num_targ_repr': 73, 'num_indisc_repr': 94}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 451, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 12, 'num_indisc_conc': 13, 'num_targ_repr': 77, 'num_indisc_repr': 80}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 5361, 'total_num_attacks': 9822, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5012224938875306, 0.5140845070422535, 0.5265588914549654, 0.4482758620689655], [0.4834710743801653, 0.4904051172707889, 0.4541284403669725, 0.512249443207127], [0.4685466377440347, 0.503448275862069, 0.5135135135135135, 0.44]], 'num_targ_conc': 90, 'num_indisc_conc': 72, 'num_targ_repr': 15, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 113, 'total_num_attacks': 2753, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'neutral', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.5, 0.4166666666666667, 0.45454545454545453, 0.5], [0.5, 0.4, 0.5, 0.36363636363636365], [0.45454545454545453, 0.5, 0.375, 0.5]], 'num_targ_conc': 46, 'num_indisc_conc': 41, 'num_targ_repr': 53, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 628, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 38, 'num_targ_repr': 48, 'num_indisc_repr': 67}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 449, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 34, 'num_targ_repr': 42, 'num_indisc_repr': 54}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 417, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 42, 'num_targ_repr': 47, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 484, 'total_num_attacks': 2345, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.37209302325581395, 0.5675675675675675, 0.45454545454545453, 0.46153846153846156], [0.6590909090909091, 0.725, 0.5853658536585366, 0.5833333333333334], [0.6, 0.35, 0.6222222222222222, 0.5652173913043478]], 'num_targ_conc': 89, 'num_indisc_conc': 63, 'num_targ_repr': 13, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 459, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 23, 'num_targ_repr': 80, 'num_indisc_repr': 91}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 8145, 'total_num_attacks': 5289, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8323943661971831, 0.8189910979228486, 0.8190184049079755, 0.7975708502024291], [0.819047619047619, 0.8044444444444444, 0.8232758620689655, 0.7703488372093024], [0.8104196816208393, 0.7713004484304933, 0.8302180685358256, 0.811217510259918]], 'num_targ_conc': 78, 'num_indisc_conc': 75, 'num_targ_repr': 13, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 424, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 34, 'num_targ_repr': 43, 'num_indisc_repr': 53}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 469, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 23, 'num_indisc_conc': 15, 'num_targ_repr': 99, 'num_indisc_repr': 64}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 31, 'num_indisc_conc': 35, 'num_targ_repr': 49, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 435, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 14, 'num_indisc_conc': 14, 'num_targ_repr': 59, 'num_indisc_repr': 68}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 539, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 40, 'num_targ_repr': 59, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 542, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 30, 'num_indisc_conc': 39, 'num_targ_repr': 38, 'num_indisc_repr': 52}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 524, 'total_num_attacks': 1967, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4358974358974359, 0.43478260869565216, 0.3488372093023256, 0.5319148936170213], [0.825, 0.42857142857142855, 0.4444444444444444, 0.7291666666666666], [0.3902439024390244, 0.4666666666666667, 0.4, 0.4594594594594595]], 'num_targ_conc': 52, 'num_indisc_conc': 38, 'num_targ_repr': 43, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 389, 'total_num_attacks': 2229, 'dominant_sentiments': [['sympathetic', 'neutral', 'anti-violence', 'neutral'], ['neutral', 'neutral', 'sympathetic', 'sympathetic'], ['sympathetic', 'neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.48484848484848486, 0.5, 0.3548387096774194, 0.5161290322580645], [0.45161290322580644, 0.5625, 0.375, 0.5121951219512195], [0.6363636363636364, 0.4, 0.4666666666666667, 0.5555555555555556]], 'num_targ_conc': 41, 'num_indisc_conc': 35, 'num_targ_repr': 43, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 424, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 42, 'num_indisc_conc': 30, 'num_targ_repr': 39, 'num_indisc_repr': 56}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 595, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 34, 'num_indisc_conc': 45, 'num_targ_repr': 46, 'num_indisc_repr': 59}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2, 'total_num_attacks': 524, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 68, 'num_targ_repr': 45, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 660, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 24, 'num_targ_repr': 68, 'num_indisc_repr': 92}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 432, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 9, 'num_targ_repr': 75, 'num_indisc_repr': 95}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 607, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 12, 'num_targ_repr': 83, 'num_indisc_repr': 88}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 65, 'num_targ_repr': 23, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 4, 'total_num_attacks': 426, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0]], 'num_targ_conc': 76, 'num_indisc_conc': 66, 'num_targ_repr': 11, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 79, 'num_indisc_conc': 76, 'num_targ_repr': 16, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 33, 'num_indisc_conc': 39, 'num_targ_repr': 47, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 446, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 52, 'num_indisc_conc': 45, 'num_targ_repr': 36, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 450, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 46, 'num_indisc_conc': 39, 'num_targ_repr': 46, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 404, 'total_num_attacks': 2256, 'dominant_sentiments': [['neutral', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4666666666666667, 0.43333333333333335, 0.4117647058823529, 0.47058823529411764], [0.4594594594594595, 0.6, 0.6129032258064516, 0.3939393939393939], [0.4358974358974359, 0.5, 0.5121951219512195, 0.48484848484848486]], 'num_targ_conc': 41, 'num_indisc_conc': 47, 'num_targ_repr': 40, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 14, 'num_targ_repr': 59, 'num_indisc_repr': 68}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 3, 'total_num_attacks': 417, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 58, 'num_indisc_conc': 80, 'num_targ_repr': 26, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 609, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 22, 'num_targ_repr': 77, 'num_indisc_repr': 88}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 777, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 64, 'num_targ_repr': 33, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 471, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 24, 'num_targ_repr': 77, 'num_indisc_repr': 84}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 409, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 16, 'num_targ_repr': 87, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 672, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 11, 'num_indisc_conc': 17, 'num_targ_repr': 79, 'num_indisc_repr': 71}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 1229, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 57, 'num_indisc_conc': 48, 'num_targ_repr': 78, 'num_indisc_repr': 48}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 731, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 51, 'num_targ_repr': 45, 'num_indisc_repr': 48}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 472, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 67, 'num_targ_repr': 34, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 25, 'num_targ_repr': 58, 'num_indisc_repr': 91}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 16, 'num_targ_repr': 76, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 501, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 66, 'num_targ_repr': 32, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 649, 'total_num_attacks': 2898, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.6, 0.6041666666666666, 0.6190476190476191, 0.5769230769230769], [0.5166666666666667, 0.6346153846153846, 0.4032258064516129, 0.5660377358490566], [0.5357142857142857, 0.5471698113207547, 0.5434782608695652, 0.43859649122807015]], 'num_targ_conc': 81, 'num_indisc_conc': 64, 'num_targ_repr': 24, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 1516, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 23, 'num_targ_repr': 59, 'num_indisc_repr': 70}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 1, 'total_num_attacks': 418, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 55, 'num_indisc_conc': 62, 'num_targ_repr': 39, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 282, 'total_num_attacks': 2250, 'dominant_sentiments': [['sympathetic', 'sympathetic', 'sympathetic', 'neutral'], ['sympathetic', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5555555555555556, 0.52, 0.5555555555555556, 0.4], [0.5333333333333333, 0.4230769230769231, 0.4782608695652174, 0.5833333333333334], [0.42857142857142855, 0.4444444444444444, 0.39285714285714285, 0.4074074074074074]], 'num_targ_conc': 23, 'num_indisc_conc': 18, 'num_targ_repr': 79, 'num_indisc_repr': 65}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 505, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 22, 'num_targ_repr': 79, 'num_indisc_repr': 93}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 458, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 14, 'num_indisc_conc': 21, 'num_targ_repr': 91, 'num_indisc_repr': 67}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 471, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 46, 'num_targ_repr': 46, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 443, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 63, 'num_targ_repr': 21, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 421, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 40, 'num_targ_repr': 39, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 491, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 42, 'num_indisc_conc': 34, 'num_targ_repr': 50, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 426, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 16, 'num_targ_repr': 86, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 445, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 41, 'num_targ_repr': 41, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 432, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 52, 'num_indisc_conc': 45, 'num_targ_repr': 45, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 469, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 21, 'num_indisc_conc': 13, 'num_targ_repr': 74, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 1, 'total_num_attacks': 442, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 78, 'num_indisc_conc': 70, 'num_targ_repr': 29, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 423, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 65, 'num_targ_repr': 23, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 1, 'total_num_attacks': 448, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 69, 'num_indisc_conc': 77, 'num_targ_repr': 31, 'num_indisc_repr': 34}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 5, 'total_num_attacks': 453, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], 'num_targ_conc': 45, 'num_indisc_conc': 63, 'num_targ_repr': 32, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 454, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 61, 'num_indisc_conc': 72, 'num_targ_repr': 41, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 390, 'total_num_attacks': 1523, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.48484848484848486, 0.8518518518518519, 0.6470588235294118, 0.6176470588235294], [0.7666666666666667, 0.45714285714285713, 0.71875, 0.6756756756756757], [0.5454545454545454, 0.5161290322580645, 0.8787878787878788, 0.5172413793103449]], 'num_targ_conc': 40, 'num_indisc_conc': 35, 'num_targ_repr': 38, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 697, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 72, 'num_indisc_conc': 72, 'num_targ_repr': 42, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 365, 'total_num_attacks': 1083, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6875, 0.7666666666666667, 0.47058823529411764, 0.5483870967741935], [0.5625, 0.7931034482758621, 0.6, 0.6333333333333333], [0.6551724137931034, 0.7419354838709677, 0.6896551724137931, 0.7142857142857143]], 'num_targ_conc': 49, 'num_indisc_conc': 62, 'num_targ_repr': 77, 'num_indisc_repr': 82}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 486, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 11, 'num_indisc_conc': 20, 'num_targ_repr': 93, 'num_indisc_repr': 86}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 430, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 12, 'num_indisc_conc': 18, 'num_targ_repr': 67, 'num_indisc_repr': 70}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 546, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 20, 'num_targ_repr': 67, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 479, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 62, 'num_indisc_conc': 65, 'num_targ_repr': 38, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 408, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 14, 'num_indisc_conc': 17, 'num_targ_repr': 77, 'num_indisc_repr': 78}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 10, 'num_indisc_conc': 14, 'num_targ_repr': 79, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 434, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 16, 'num_targ_repr': 78, 'num_indisc_repr': 66}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 434, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 14, 'num_indisc_conc': 17, 'num_targ_repr': 65, 'num_indisc_repr': 71}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 465, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 86, 'num_indisc_conc': 61, 'num_targ_repr': 21, 'num_indisc_repr': 34}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 841, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 59, 'num_indisc_conc': 76, 'num_targ_repr': 33, 'num_indisc_repr': 46}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 9, 'num_indisc_conc': 17, 'num_targ_repr': 70, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 654, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 23, 'num_targ_repr': 72, 'num_indisc_repr': 84}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 421, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 44, 'num_indisc_conc': 37, 'num_targ_repr': 49, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 481, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 39, 'num_targ_repr': 45, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 463, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 65, 'num_indisc_conc': 70, 'num_targ_repr': 37, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 427, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 54, 'num_targ_repr': 44, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 31, 'num_indisc_conc': 32, 'num_targ_repr': 51, 'num_indisc_repr': 54}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 421, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 42, 'num_indisc_conc': 27, 'num_targ_repr': 38, 'num_indisc_repr': 48}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 431, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 39, 'num_targ_repr': 44, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 11, 'total_num_attacks': 499, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 54, 'num_indisc_conc': 82, 'num_targ_repr': 31, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 518, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 13, 'num_indisc_conc': 13, 'num_targ_repr': 87, 'num_indisc_repr': 95}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 411, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 63, 'num_indisc_conc': 74, 'num_targ_repr': 19, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 451, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 68, 'num_indisc_conc': 68, 'num_targ_repr': 17, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 417, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 73, 'num_targ_repr': 26, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 423, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 82, 'num_targ_repr': 31, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 18, 'num_targ_repr': 58, 'num_indisc_repr': 69}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 798, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 23, 'num_indisc_conc': 29, 'num_targ_repr': 67, 'num_indisc_repr': 88}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 511, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 81, 'num_indisc_conc': 73, 'num_targ_repr': 16, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 454, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 45, 'num_targ_repr': 53, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 381, 'total_num_attacks': 2272, 'dominant_sentiments': [['neutral', 'sympathetic', 'neutral', 'anti-violence'], ['sympathetic', 'sympathetic', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.5862068965517241, 0.59375, 0.53125, 0.325], [0.5428571428571428, 0.40625, 0.6551724137931034, 0.43333333333333335], [0.5405405405405406, 0.5333333333333333, 0.40625, 0.4666666666666667]], 'num_targ_conc': 33, 'num_indisc_conc': 39, 'num_targ_repr': 40, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 475, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 45, 'num_indisc_conc': 44, 'num_targ_repr': 45, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 410, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 36, 'num_targ_repr': 37, 'num_indisc_repr': 46}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 421, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 23, 'num_indisc_conc': 12, 'num_targ_repr': 64, 'num_indisc_repr': 78}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2, 'total_num_attacks': 479, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 59, 'num_indisc_conc': 68, 'num_targ_repr': 28, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 354, 'total_num_attacks': 2188, 'dominant_sentiments': [['anti-violence', 'neutral', 'sympathetic', 'neutral'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['anti-violence', 'anti-violence', 'sympathetic', 'anti-violence']], 'percent_dominant_sentiments': [[0.4838709677419355, 0.4827586206896552, 0.5666666666666667, 0.625], [0.5862068965517241, 0.5588235294117647, 0.41935483870967744, 0.45454545454545453], [0.5833333333333334, 0.7083333333333334, 0.8076923076923077, 0.6206896551724138]], 'num_targ_conc': 43, 'num_indisc_conc': 35, 'num_targ_repr': 42, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 64, 'num_indisc_conc': 76, 'num_targ_repr': 25, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 452, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 68, 'num_indisc_repr': 87}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 348, 'total_num_attacks': 1717, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.42424242424242425, 0.6428571428571429, 0.7037037037037037, 0.6296296296296297], [0.6451612903225806, 0.7575757575757576, 0.5862068965517241, 0.7777777777777778], [0.5862068965517241, 0.68, 0.4838709677419355, 0.5161290322580645]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 70, 'num_indisc_repr': 72}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 492, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 22, 'num_indisc_conc': 12, 'num_targ_repr': 57, 'num_indisc_repr': 66}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 430, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 43, 'num_targ_repr': 37, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 550, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 47, 'num_targ_repr': 43, 'num_indisc_repr': 57}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 423, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 60, 'num_indisc_conc': 80, 'num_targ_repr': 24, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 419, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 75, 'num_targ_repr': 28, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 419, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 62, 'num_indisc_conc': 55, 'num_targ_repr': 21, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 2, 'total_num_attacks': 414, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 61, 'num_indisc_conc': 80, 'num_targ_repr': 30, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 40, 'num_targ_repr': 48, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 412, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 55, 'num_indisc_conc': 61, 'num_targ_repr': 26, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 14, 'num_targ_repr': 83, 'num_indisc_repr': 85}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 427, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 47, 'num_indisc_conc': 48, 'num_targ_repr': 49, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 72, 'num_indisc_conc': 73, 'num_targ_repr': 27, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 373, 'total_num_attacks': 2131, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'neutral', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.4666666666666667, 0.3939393939393939, 0.47058823529411764, 0.71875], [0.37037037037037035, 0.9285714285714286, 0.7586206896551724, 0.8214285714285714], [0.3793103448275862, 0.5, 0.5757575757575758, 0.4375]], 'num_targ_conc': 51, 'num_indisc_conc': 40, 'num_targ_repr': 38, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 436, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 18, 'num_targ_repr': 68, 'num_indisc_repr': 82}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 472, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 73, 'num_indisc_conc': 76, 'num_targ_repr': 23, 'num_indisc_repr': 34}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 453, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 66, 'num_targ_repr': 29, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 445, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 71, 'num_targ_repr': 24, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 461, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 36, 'num_targ_repr': 49, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 432, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 16, 'num_targ_repr': 65, 'num_indisc_repr': 77}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 442, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 22, 'num_indisc_conc': 16, 'num_targ_repr': 86, 'num_indisc_repr': 76}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 60, 'total_num_attacks': 996, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8333333333333334, 1.0, 1.0, 1.0], [0.8, 0.75, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0]], 'num_targ_conc': 49, 'num_indisc_conc': 56, 'num_targ_repr': 83, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 19, 'num_targ_repr': 87, 'num_indisc_repr': 61}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 517, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 29, 'num_targ_repr': 68, 'num_indisc_repr': 89}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 442, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 18, 'num_targ_repr': 73, 'num_indisc_repr': 87}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 510, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 45, 'num_targ_repr': 51, 'num_indisc_repr': 56}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 425, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 75, 'num_indisc_repr': 79}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 497, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 22, 'num_indisc_conc': 15, 'num_targ_repr': 65, 'num_indisc_repr': 77}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 35, 'total_num_attacks': 2104, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'neutral', 'anti-violence'], ['neutral', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 0.75, 0.5, 1.0], [0.6666666666666666, 0.4, 0.75, 0.75], [1.0, 0.5, 1.0, 0.5]], 'num_targ_conc': 19, 'num_indisc_conc': 15, 'num_targ_repr': 71, 'num_indisc_repr': 77}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 413, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 44, 'num_targ_repr': 41, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 439, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 80, 'num_indisc_conc': 72, 'num_targ_repr': 25, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 492, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 66, 'num_indisc_conc': 64, 'num_targ_repr': 40, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 472, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 74, 'num_indisc_repr': 85}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 434, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 44, 'num_targ_repr': 56, 'num_indisc_repr': 46}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 368, 'total_num_attacks': 2290, 'dominant_sentiments': [['anti-violence', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'neutral', 'sympathetic', 'sympathetic'], ['sympathetic', 'sympathetic', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.46875, 0.43333333333333335, 0.45161290322580644, 0.36363636363636365], [0.5666666666666667, 0.41935483870967744, 0.48484848484848486, 0.5666666666666667], [0.4482758620689655, 0.4642857142857143, 0.41935483870967744, 0.46875]], 'num_targ_conc': 35, 'num_indisc_conc': 34, 'num_targ_repr': 48, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 438, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 44, 'num_indisc_conc': 45, 'num_targ_repr': 47, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 5, 'total_num_attacks': 468, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 69, 'num_indisc_conc': 71, 'num_targ_repr': 33, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 462, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 73, 'num_indisc_conc': 75, 'num_targ_repr': 18, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 445, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 21, 'num_targ_repr': 77, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 42, 'num_targ_repr': 44, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 466, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 31, 'num_targ_repr': 57, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 397, 'total_num_attacks': 2276, 'dominant_sentiments': [['neutral', 'neutral', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.40625, 0.5, 0.45161290322580644, 0.45161290322580644], [0.375, 0.45714285714285713, 0.375, 0.42857142857142855], [0.5357142857142857, 0.6, 0.46875, 0.5862068965517241]], 'num_targ_conc': 13, 'num_indisc_conc': 15, 'num_targ_repr': 80, 'num_indisc_repr': 60}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 978, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 17, 'num_targ_repr': 67, 'num_indisc_repr': 74}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 369, 'total_num_attacks': 2272, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4, 0.5625, 0.6666666666666666, 0.5483870967741935], [0.45161290322580644, 0.8275862068965517, 0.4, 0.4411764705882353], [0.5862068965517241, 0.5, 0.6176470588235294, 0.4]], 'num_targ_conc': 19, 'num_indisc_conc': 16, 'num_targ_repr': 70, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 487, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 20, 'num_indisc_conc': 17, 'num_targ_repr': 78, 'num_indisc_repr': 84}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 365, 'total_num_attacks': 2137, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'neutral', 'sympathetic'], ['neutral', 'sympathetic', 'sympathetic', 'neutral'], ['neutral', 'sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.36666666666666664, 1.0, 0.53125, 0.5], [0.5151515151515151, 0.6296296296296297, 0.48484848484848486, 0.4838709677419355], [0.37142857142857144, 0.53125, 0.5925925925925926, 0.5517241379310345]], 'num_targ_conc': 40, 'num_indisc_conc': 37, 'num_targ_repr': 30, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 1, 'total_num_attacks': 439, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 87, 'num_indisc_conc': 68, 'num_targ_repr': 21, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 420, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 70, 'num_indisc_conc': 83, 'num_targ_repr': 20, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 19, 'num_targ_repr': 78, 'num_indisc_repr': 86}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 434, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 33, 'num_indisc_conc': 48, 'num_targ_repr': 39, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 440, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 19, 'num_indisc_conc': 20, 'num_targ_repr': 81, 'num_indisc_repr': 59}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 441, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 66, 'num_indisc_conc': 71, 'num_targ_repr': 21, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 451, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 40, 'num_targ_repr': 35, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 432, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 65, 'num_indisc_conc': 76, 'num_targ_repr': 24, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 424, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 42, 'num_indisc_conc': 46, 'num_targ_repr': 50, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 56, 'num_indisc_conc': 68, 'num_targ_repr': 30, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 433, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 37, 'num_targ_repr': 56, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 424, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 40, 'num_targ_repr': 49, 'num_indisc_repr': 58}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 459, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 36, 'num_targ_repr': 49, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 712, 'total_num_attacks': 2684, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6610169491525424, 0.7121212121212122, 0.851063829787234, 0.717391304347826], [0.7692307692307693, 0.65625, 0.75, 0.6615384615384615], [0.7205882352941176, 0.7288135593220338, 0.7719298245614035, 0.6825396825396826]], 'num_targ_conc': 65, 'num_indisc_conc': 72, 'num_targ_repr': 19, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 459, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 17, 'num_targ_repr': 92, 'num_indisc_repr': 62}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 66, 'num_indisc_conc': 89, 'num_targ_repr': 25, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 629, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 19, 'num_targ_repr': 67, 'num_indisc_repr': 74}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 419, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 41, 'num_targ_repr': 44, 'num_indisc_repr': 53}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 429, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 19, 'num_targ_repr': 81, 'num_indisc_repr': 71}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 495, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 30, 'num_indisc_conc': 40, 'num_targ_repr': 66, 'num_indisc_repr': 62}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 507, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 49, 'num_indisc_conc': 33, 'num_targ_repr': 49, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 441, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 34, 'num_indisc_conc': 42, 'num_targ_repr': 51, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 41, 'num_targ_repr': 44, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 648, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 28, 'num_indisc_conc': 34, 'num_targ_repr': 41, 'num_indisc_repr': 62}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 10, 'total_num_attacks': 685, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 48, 'num_targ_repr': 46, 'num_indisc_repr': 53}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 15, 'num_indisc_conc': 16, 'num_targ_repr': 77, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 530, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 33, 'num_indisc_conc': 40, 'num_targ_repr': 53, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 435, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 34, 'num_indisc_conc': 38, 'num_targ_repr': 53, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 427, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 76, 'num_indisc_conc': 69, 'num_targ_repr': 24, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 423, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 40, 'num_indisc_conc': 40, 'num_targ_repr': 48, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 120, 'total_num_attacks': 1297, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.75, 0.6, 0.7647058823529411, 0.8333333333333334], [0.5, 0.5384615384615384, 0.7857142857142857, 0.6], [0.5, 1.0, 0.5714285714285714, 0.7857142857142857]], 'num_targ_conc': 20, 'num_indisc_conc': 13, 'num_targ_repr': 78, 'num_indisc_repr': 77}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 438, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 33, 'num_targ_repr': 60, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 277, 'total_num_attacks': 1415, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8333333333333334, 0.8636363636363636, 0.8636363636363636, 0.9655172413793104], [0.8888888888888888, 0.8947368421052632, 0.85, 0.8181818181818182], [0.8333333333333334, 0.75, 0.8888888888888888, 0.7575757575757576]], 'num_targ_conc': 15, 'num_indisc_conc': 19, 'num_targ_repr': 62, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 438, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 63, 'num_indisc_conc': 70, 'num_targ_repr': 29, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 423, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 34, 'num_targ_repr': 53, 'num_indisc_repr': 52}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 594, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 45, 'num_targ_repr': 54, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 482, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 39, 'num_indisc_conc': 32, 'num_targ_repr': 52, 'num_indisc_repr': 57}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 448, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 71, 'num_indisc_conc': 61, 'num_targ_repr': 35, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 400}, 'final_pop': 0, 'total_num_attacks': 417, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 68, 'num_indisc_conc': 63, 'num_targ_repr': 41, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 400}, 'final_pop': 4, 'total_num_attacks': 416, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 82, 'num_indisc_conc': 72, 'num_targ_repr': 15, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 1101, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 15, 'num_targ_repr': 38, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, 'final_pop': 183, 'total_num_attacks': 803, 'dominant_sentiments': [['sympathetic', 'sympathetic', 'neutral'], ['anti-violence', 'sympathetic', 'neutral']], 'percent_dominant_sentiments': [[0.3225806451612903, 0.4827586206896552, 0.5], [0.5333333333333333, 0.5806451612903226, 0.37037037037037035]], 'num_targ_conc': 32, 'num_indisc_conc': 34, 'num_targ_repr': 33, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 392, 'total_num_attacks': 945, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7297297297297297, 0.75, 0.967741935483871, 0.8823529411764706], [0.967741935483871, 0.8055555555555556, 0.5714285714285714, 0.5945945945945946], [0.9411764705882353, 0.8571428571428571, 0.9666666666666667, 0.8148148148148148]], 'num_targ_conc': 19, 'num_indisc_conc': 12, 'num_targ_repr': 21, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, 'final_pop': 629, 'total_num_attacks': 1787, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6774193548387096, 0.75, 0.7272727272727273, 0.7096774193548387, 0.5142857142857142], [0.7333333333333333, 0.4482758620689655, 0.8333333333333334, 0.5172413793103449, 0.6333333333333333], [0.6333333333333333, 0.6774193548387096, 0.7428571428571429, 0.8571428571428571, 0.7272727272727273], [0.5588235294117647, 0.7586206896551724, 0.625, 0.5517241379310345, 0.6451612903225806]], 'num_targ_conc': 25, 'num_indisc_conc': 27, 'num_targ_repr': 34, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 760, 'total_num_attacks': 2259, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'anti-violence', 'neutral', 'sympathetic'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'sympathetic', 'anti-violence', 'neutral'], ['sympathetic', 'sympathetic', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.45161290322580644, 0.5483870967741935, 0.6666666666666666, 0.4838709677419355, 0.5161290322580645], [0.4642857142857143, 0.5517241379310345, 0.45454545454545453, 0.5483870967741935, 0.625], [0.9333333333333333, 0.6, 0.4827586206896552, 0.5517241379310345, 0.4827586206896552], [0.43333333333333335, 0.46875, 0.5185185185185185, 0.5517241379310345, 0.7931034482758621], [0.6666666666666666, 0.625, 0.5483870967741935, 0.3870967741935484, 0.7]], 'num_targ_conc': 18, 'num_indisc_conc': 13, 'num_targ_repr': 23, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 759, 'total_num_attacks': 3043, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5151515151515151, 0.6538461538461539, 0.8620689655172413, 0.6206896551724138, 0.6470588235294118], [0.5483870967741935, 0.7333333333333333, 0.5882352941176471, 0.40625, 0.6060606060606061], [0.6551724137931034, 0.45454545454545453, 0.6666666666666666, 0.8666666666666667, 0.6774193548387096], [0.7, 0.6875, 0.4375, 0.48484848484848486, 0.9230769230769231], [0.6428571428571429, 0.6333333333333333, 0.5333333333333333, 0.9666666666666667, 0.5333333333333333]], 'num_targ_conc': 21, 'num_indisc_conc': 25, 'num_targ_repr': 32, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, 'final_pop': 834, 'total_num_attacks': 2147, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic', 'neutral', 'neutral'], ['sympathetic', 'neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.5384615384615384, 0.4375, 0.5666666666666667, 0.4666666666666667, 0.4166666666666667], [0.45454545454545453, 0.45161290322580644, 0.5405405405405406, 0.5, 0.4375], [0.42857142857142855, 0.5588235294117647, 0.41935483870967744, 0.4857142857142857, 0.6129032258064516], [0.5588235294117647, 0.5, 0.3870967741935484, 0.41025641025641024, 0.41379310344827586], [0.4864864864864865, 0.5428571428571428, 0.6060606060606061, 0.5, 0.4166666666666667]], 'num_targ_conc': 14, 'num_indisc_conc': 28, 'num_targ_repr': 21, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 330, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 123, 'num_indisc_conc': 124, 'num_targ_repr': 31, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 657, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 47, 'num_indisc_conc': 52, 'num_targ_repr': 21, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 239, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 53, 'num_targ_repr': 21, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 817, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 27, 'num_indisc_conc': 33, 'num_targ_repr': 27, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 677, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 98, 'num_indisc_conc': 131, 'num_targ_repr': 29, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, 'final_pop': 2, 'total_num_attacks': 357, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0], [0.0, 0.0, 0.0], [1.0, 0.0, 0.0]], 'num_targ_conc': 34, 'num_indisc_conc': 40, 'num_targ_repr': 17, 'num_indisc_repr': 14}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 643, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 65, 'num_indisc_conc': 55, 'num_targ_repr': 22, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, 'final_pop': 1, 'total_num_attacks': 905, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 31, 'num_targ_repr': 9, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 973, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 90, 'num_indisc_conc': 120, 'num_targ_repr': 52, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, 'final_pop': 3, 'total_num_attacks': 394, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 1.0], [1.0, 0.0, 0.0]], 'num_targ_conc': 77, 'num_indisc_conc': 84, 'num_targ_repr': 46, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 674, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 57, 'num_indisc_conc': 48, 'num_targ_repr': 22, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, 'final_pop': 2, 'total_num_attacks': 840, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 1.0, 0.0]], 'num_targ_conc': 123, 'num_indisc_conc': 126, 'num_targ_repr': 61, 'num_indisc_repr': 60}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, 'final_pop': 4, 'total_num_attacks': 440, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 61, 'num_indisc_conc': 58, 'num_targ_repr': 18, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 1000, 'total_num_attacks': 3128, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8333333333333334, 0.8157894736842105, 0.8205128205128205, 0.8947368421052632, 0.8108108108108109], [0.8863636363636364, 0.9354838709677419, 0.8918918918918919, 0.9230769230769231, 0.8636363636363636], [0.8947368421052632, 0.813953488372093, 0.8947368421052632, 0.9459459459459459, 0.8536585365853658], [0.9210526315789473, 0.9024390243902439, 0.8666666666666667, 0.8571428571428571, 0.9230769230769231], [0.8461538461538461, 0.9111111111111111, 0.8947368421052632, 0.8372093023255814, 0.9]], 'num_targ_conc': 44, 'num_indisc_conc': 50, 'num_targ_repr': 15, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, 'final_pop': 1083, 'total_num_attacks': 956, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4247787610619469, 0.5405405405405406, 0.4580152671755725], [0.45, 0.42016806722689076, 0.4369747899159664], [0.4274193548387097, 0.4537037037037037, 0.4796747967479675]], 'num_targ_conc': 42, 'num_indisc_conc': 37, 'num_targ_repr': 4, 'num_indisc_repr': 9}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 1932, 'total_num_attacks': 1099, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7073170731707317, 0.6909090909090909, 0.7631578947368421, 0.6833333333333333], [0.726027397260274, 0.6538461538461539, 0.7588235294117647, 0.7352941176470589], [0.756578947368421, 0.6643835616438356, 0.7619047619047619, 0.7402597402597403]], 'num_targ_conc': 31, 'num_indisc_conc': 44, 'num_targ_repr': 6, 'num_indisc_repr': 8}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 889, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 11, 'num_indisc_conc': 19, 'num_targ_repr': 43, 'num_indisc_repr': 53}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 274, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 11, 'num_indisc_conc': 8, 'num_targ_repr': 39, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 135, 'total_num_attacks': 1466, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8888888888888888, 1.0, 1.0, 0.9, 1.0], [0.8333333333333334, 0.75, 0.8888888888888888, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0], [0.7142857142857143, 1.0, 0.8181818181818182, 1.0, 1.0]], 'num_targ_conc': 9, 'num_indisc_conc': 5, 'num_targ_repr': 30, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 566, 'total_num_attacks': 2642, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5172413793103449, 0.6, 0.7142857142857143, 0.8333333333333334, 0.5833333333333334], [0.4583333333333333, 0.8571428571428571, 0.8, 0.7391304347826086, 0.4074074074074074], [0.75, 0.7777777777777778, 0.8846153846153846, 0.7727272727272727, 0.5862068965517241], [0.75, 0.7222222222222222, 0.96, 0.7692307692307693, 0.7894736842105263], [0.8181818181818182, 0.5, 0.4444444444444444, 0.7666666666666667, 0.9444444444444444]], 'num_targ_conc': 11, 'num_indisc_conc': 9, 'num_targ_repr': 69, 'num_indisc_repr': 57}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, 'final_pop': 121, 'total_num_attacks': 896, 'dominant_sentiments': [['sympathetic', 'neutral', 'sympathetic'], ['anti-violence', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.42857142857142855, 0.625, 0.5454545454545454], [0.4166666666666667, 0.4375, 0.47058823529411764], [0.5384615384615384, 0.42857142857142855, 0.4166666666666667]], 'num_targ_conc': 7, 'num_indisc_conc': 8, 'num_targ_repr': 30, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, 'final_pop': 4, 'total_num_attacks': 4322, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'sympathetic', 'anti-violence'], ['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 23, 'num_indisc_conc': 18, 'num_targ_repr': 81, 'num_indisc_repr': 97}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, 'final_pop': 842, 'total_num_attacks': 3378, 'dominant_sentiments': [['sympathetic', 'sympathetic', 'neutral', 'neutral', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'sympathetic', 'neutral'], ['sympathetic', 'sympathetic', 'sympathetic', 'sympathetic', 'neutral'], ['neutral', 'sympathetic', 'neutral', 'sympathetic', 'sympathetic'], ['neutral', 'sympathetic', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.4444444444444444, 0.5945945945945946, 0.4482758620689655, 0.6857142857142857, 0.5263157894736842], [0.4166666666666667, 0.5263157894736842, 0.5714285714285714, 0.5263157894736842, 0.4166666666666667], [0.46875, 0.41379310344827586, 0.42857142857142855, 0.3939393939393939, 0.5], [0.4411764705882353, 0.5, 0.45454545454545453, 0.4827586206896552, 0.45714285714285713], [0.4838709677419355, 0.4375, 0.5675675675675675, 0.4594594594594595, 0.4166666666666667]], 'num_targ_conc': 21, 'num_indisc_conc': 12, 'num_targ_repr': 49, 'num_indisc_repr': 48}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 79, 'total_num_attacks': 683, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7777777777777778, 0.8666666666666667, 0.6666666666666666], [0.9, 0.7857142857142857, 0.8333333333333334]], 'num_targ_conc': 15, 'num_indisc_conc': 17, 'num_targ_repr': 44, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 348, 'total_num_attacks': 693, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.3958333333333333, 0.8529411764705882, 0.7647058823529411], [0.5652173913043478, 0.6388888888888888, 0.5714285714285714], [0.4117647058823529, 0.5714285714285714, 0.5555555555555556]], 'num_targ_conc': 14, 'num_indisc_conc': 11, 'num_targ_repr': 63, 'num_indisc_repr': 50}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 1011, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 8, 'num_indisc_conc': 11, 'num_targ_repr': 57, 'num_indisc_repr': 61}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 1379, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 8, 'num_indisc_conc': 6, 'num_targ_repr': 41, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 744, 'total_num_attacks': 2302, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'sympathetic', 'anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.5, 0.4482758620689655, 0.38461538461538464, 0.4722222222222222, 0.45161290322580644], [0.4482758620689655, 0.41379310344827586, 0.5185185185185185, 0.39285714285714285, 0.5], [0.4594594594594595, 0.53125, 0.4827586206896552, 0.4666666666666667, 0.44], [0.5357142857142857, 0.36, 0.7241379310344828, 0.4375, 0.4444444444444444], [0.3448275862068966, 0.45161290322580644, 0.5, 0.5172413793103449, 0.40625]], 'num_targ_conc': 13, 'num_indisc_conc': 14, 'num_targ_repr': 55, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, 'final_pop': 8, 'total_num_attacks': 379, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 0.0], [1.0, 0.5, 1.0]], 'num_targ_conc': 26, 'num_indisc_conc': 25, 'num_targ_repr': 65, 'num_indisc_repr': 60}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 628, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 23, 'num_targ_repr': 18, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 446, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 23, 'num_targ_repr': 45, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, 'final_pop': 77, 'total_num_attacks': 1466, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.3333333333333333, 0.3333333333333333, 1.0, 0.5, 1.0], [0.8, 1.0, 1.0, 0.6666666666666666, 1.0], [0.6666666666666666, 0.5, 0.4, 0.3333333333333333, 0.6666666666666666], [0.5, 0.5, 1.0, 1.0, 1.0], [0.6666666666666666, 0.5, 1.0, 0.7142857142857143, 0.6666666666666666]], 'num_targ_conc': 16, 'num_indisc_conc': 17, 'num_targ_repr': 42, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 234, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 29, 'num_targ_repr': 32, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, 'final_pop': 191, 'total_num_attacks': 425, 'dominant_sentiments': [['anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8928571428571429, 0.39473684210526316, 0.8333333333333334], [0.696969696969697, 0.7878787878787878, 0.84375]], 'num_targ_conc': 19, 'num_indisc_conc': 21, 'num_targ_repr': 22, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, 'final_pop': 169, 'total_num_attacks': 1336, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'sympathetic'], ['sympathetic', 'sympathetic', 'sympathetic']], 'percent_dominant_sentiments': [[0.44, 0.43333333333333335, 0.5142857142857142], [0.4230769230769231, 0.5714285714285714, 0.4827586206896552]], 'num_targ_conc': 38, 'num_indisc_conc': 43, 'num_targ_repr': 45, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, 'final_pop': 288, 'total_num_attacks': 1120, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.3333333333333333, 0.5161290322580645, 0.5428571428571428], [0.5161290322580645, 0.7, 0.65625], [0.36363636363636365, 1.0, 0.4838709677419355]], 'num_targ_conc': 33, 'num_indisc_conc': 24, 'num_targ_repr': 33, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, 'final_pop': 361, 'total_num_attacks': 1637, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'sympathetic', 'anti-violence'], ['neutral', 'neutral', 'anti-violence', 'anti-violence'], ['neutral', 'neutral', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5, 0.6206896551724138, 0.6129032258064516, 0.3939393939393939], [0.3888888888888889, 0.6428571428571429, 0.6428571428571429, 0.56], [0.3870967741935484, 0.5357142857142857, 0.4857142857142857, 0.5]], 'num_targ_conc': 32, 'num_indisc_conc': 34, 'num_targ_repr': 22, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, 'final_pop': 376, 'total_num_attacks': 3478, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'neutral'], ['neutral', 'sympathetic', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7058823529411765, 0.4838709677419355, 0.8055555555555556, 0.5862068965517241], [0.53125, 0.7741935483870968, 0.8709677419354839, 0.5], [0.6333333333333333, 0.40625, 0.7575757575757576, 0.4375]], 'num_targ_conc': 55, 'num_indisc_conc': 68, 'num_targ_repr': 69, 'num_indisc_repr': 69}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 725, 'total_num_attacks': 2097, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7741935483870968, 0.75, 0.7096774193548387, 0.7333333333333333, 0.45161290322580644], [0.6206896551724138, 0.6666666666666666, 0.7586206896551724, 0.6296296296296297, 0.375], [0.5, 0.625, 0.76, 0.5, 0.75], [0.8846153846153846, 0.8620689655172413, 0.7241379310344828, 0.7272727272727273, 0.5], [0.4411764705882353, 0.4411764705882353, 0.76, 0.6333333333333333, 0.7419354838709677]], 'num_targ_conc': 13, 'num_indisc_conc': 19, 'num_targ_repr': 14, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 169, 'total_num_attacks': 550, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.7, 0.7777777777777778, 0.8076923076923077], [0.5483870967741935, 0.7428571428571429, 0.41379310344827586]], 'num_targ_conc': 16, 'num_indisc_conc': 14, 'num_targ_repr': 18, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, 'final_pop': 187, 'total_num_attacks': 848, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic'], ['sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4666666666666667, 0.40625, 0.5294117647058824], [0.45161290322580644, 0.45454545454545453, 0.4375]], 'num_targ_conc': 24, 'num_indisc_conc': 31, 'num_targ_repr': 22, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 200, 'total_num_attacks': 2093, 'dominant_sentiments': [['neutral', 'anti-violence', 'sympathetic'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.34285714285714286, 0.48484848484848486, 0.5142857142857142], [0.4375, 0.5, 0.4]], 'num_targ_conc': 50, 'num_indisc_conc': 68, 'num_targ_repr': 70, 'num_indisc_repr': 64}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, 'final_pop': 249, 'total_num_attacks': 1377, 'dominant_sentiments': [['neutral', 'sympathetic', 'sympathetic'], ['neutral', 'sympathetic', 'sympathetic'], ['sympathetic', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.43333333333333335, 0.46875, 0.4166666666666667], [0.34615384615384615, 0.39285714285714285, 0.46875], [0.41379310344827586, 0.4090909090909091, 0.5357142857142857]], 'num_targ_conc': 29, 'num_indisc_conc': 27, 'num_targ_repr': 23, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, 'final_pop': 421, 'total_num_attacks': 2851, 'dominant_sentiments': [['neutral', 'sympathetic', 'anti-violence', 'anti-violence'], ['sympathetic', 'neutral', 'anti-violence', 'sympathetic'], ['neutral', 'sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.5, 0.5714285714285714, 0.42424242424242425, 0.625], [0.5609756097560976, 0.5714285714285714, 0.4666666666666667, 0.46875], [0.42424242424242425, 0.4473684210526316, 0.4864864864864865, 0.40540540540540543]], 'num_targ_conc': 53, 'num_indisc_conc': 42, 'num_targ_repr': 39, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 578, 'total_num_attacks': 1921, 'dominant_sentiments': [['sympathetic', 'sympathetic', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'neutral', 'sympathetic', 'sympathetic'], ['neutral', 'neutral', 'sympathetic', 'neutral', 'neutral'], ['sympathetic', 'neutral', 'neutral', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.4375, 0.4444444444444444, 0.4827586206896552, 0.6206896551724138, 0.4444444444444444], [0.5714285714285714, 0.4838709677419355, 0.3793103448275862, 0.5454545454545454, 0.5333333333333333], [0.45161290322580644, 0.44, 0.38235294117647056, 0.4444444444444444, 0.36666666666666664], [0.4, 0.5161290322580645, 0.5172413793103449, 0.5172413793103449, 0.43333333333333335]], 'num_targ_conc': 20, 'num_indisc_conc': 18, 'num_targ_repr': 22, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 284, 'total_num_attacks': 1215, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'neutral'], ['anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.41379310344827586, 0.5806451612903226, 0.41379310344827586], [0.4864864864864865, 0.9285714285714286, 0.375], [0.8275862068965517, 0.45161290322580644, 0.4375]], 'num_targ_conc': 24, 'num_indisc_conc': 26, 'num_targ_repr': 34, 'num_indisc_repr': 22}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 9, 'total_num_attacks': 631, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 41, 'num_targ_repr': 16, 'num_indisc_repr': 10}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 251, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 49, 'num_targ_repr': 42, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 200, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 235, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 82, 'num_indisc_conc': 68, 'num_targ_repr': 52, 'num_indisc_repr': 34}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 1, 'total_num_attacks': 232, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 29, 'num_targ_repr': 20, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, 'final_pop': 13, 'total_num_attacks': 397, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.75, 1.0, 1.0], [0.0, 0.0, 1.0], [0.0, 1.0, 1.0]], 'num_targ_conc': 122, 'num_indisc_conc': 131, 'num_targ_repr': 37, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, 'final_pop': 2, 'total_num_attacks': 1988, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 111, 'num_indisc_conc': 114, 'num_targ_repr': 87, 'num_indisc_repr': 66}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, 'final_pop': 490, 'total_num_attacks': 1442, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7692307692307693, 0.9, 0.813953488372093, 0.813953488372093], [0.6521739130434783, 0.7142857142857143, 0.8048780487804879, 0.7894736842105263], [0.7631578947368421, 0.717948717948718, 0.8048780487804879, 0.8918918918918919]], 'num_targ_conc': 56, 'num_indisc_conc': 49, 'num_targ_repr': 12, 'num_indisc_repr': 9}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 762, 'total_num_attacks': 3358, 'dominant_sentiments': [['neutral', 'sympathetic', 'sympathetic'], ['neutral', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.4375, 0.4305555555555556, 0.37681159420289856], [0.4589041095890411, 0.4174757281553398, 0.4015748031496063]], 'num_targ_conc': 125, 'num_indisc_conc': 127, 'num_targ_repr': 26, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, 'final_pop': 828, 'total_num_attacks': 4687, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8269230769230769, 0.7818181818181819, 0.8241758241758241], [0.7931034482758621, 0.8144329896907216, 0.7831325301204819], [0.8279569892473119, 0.8192771084337349, 0.8541666666666666]], 'num_targ_conc': 81, 'num_indisc_conc': 96, 'num_targ_repr': 18, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, 'final_pop': 1023, 'total_num_attacks': 5884, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.35555555555555557, 0.4470588235294118, 0.45454545454545453, 0.3875], [0.3548387096774194, 0.3953488372093023, 0.3372093023255814, 0.3793103448275862], [0.3888888888888889, 0.41284403669724773, 0.40860215053763443, 0.375]], 'num_targ_conc': 104, 'num_indisc_conc': 94, 'num_targ_repr': 20, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 2, 'total_num_attacks': 351, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0], [1.0, 0.0, 0.0]], 'num_targ_conc': 21, 'num_indisc_conc': 17, 'num_targ_repr': 43, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 933, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 26, 'num_indisc_conc': 28, 'num_targ_repr': 122, 'num_indisc_repr': 128}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 199, 'total_num_attacks': 1141, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.5555555555555556, 0.5, 1.0, 0.5, 0.5], [0.75, 1.0, 0.4, 0.5, 0.5555555555555556], [0.6666666666666666, 0.7, 0.5, 0.7, 0.7], [0.6, 0.6666666666666666, 0.875, 0.4, 0.7777777777777778], [0.5714285714285714, 0.5, 0.6923076923076923, 0.6666666666666666, 0.5714285714285714]], 'num_targ_conc': 33, 'num_indisc_conc': 32, 'num_targ_repr': 53, 'num_indisc_repr': 54}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, 'final_pop': 316, 'total_num_attacks': 1233, 'dominant_sentiments': [['anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7058823529411765, 0.4482758620689655, 0.9375], [0.5, 0.5526315789473685, 0.45454545454545453], [0.35, 0.696969696969697, 0.5714285714285714]], 'num_targ_conc': 24, 'num_indisc_conc': 37, 'num_targ_repr': 34, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700}, 'final_pop': 278, 'total_num_attacks': 2608, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.71875, 0.6666666666666666, 0.7666666666666667], [0.3870967741935484, 0.6764705882352942, 0.5862068965517241], [0.7419354838709677, 0.7647058823529411, 0.6071428571428571]], 'num_targ_conc': 82, 'num_indisc_conc': 67, 'num_targ_repr': 54, 'num_indisc_repr': 65}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 801, 'total_num_attacks': 2040, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.53125, 0.75, 0.59375, 0.8333333333333334, 0.4117647058823529], [0.9117647058823529, 0.7058823529411765, 0.7586206896551724, 0.7575757575757576, 0.4166666666666667], [0.8064516129032258, 0.5151515151515151, 0.45454545454545453, 0.45714285714285713, 0.6129032258064516], [0.4666666666666667, 0.7647058823529411, 0.4444444444444444, 0.8709677419354839, 0.3939393939393939], [0.6666666666666666, 0.45454545454545453, 0.5769230769230769, 0.4666666666666667, 0.4642857142857143]], 'num_targ_conc': 20, 'num_indisc_conc': 21, 'num_targ_repr': 20, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 777, 'total_num_attacks': 3293, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['anti-violence', 'neutral', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'sympathetic', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.41935483870967744, 0.5, 0.896551724137931, 0.6071428571428571, 0.4], [0.5, 0.8666666666666667, 0.7058823529411765, 0.375, 0.45161290322580644], [0.48484848484848486, 0.4, 0.6875, 0.41935483870967744, 0.9310344827586207], [0.7407407407407407, 0.75, 0.5625, 0.7, 0.8387096774193549], [0.3939393939393939, 0.40625, 0.375, 0.35714285714285715, 0.8709677419354839]], 'num_targ_conc': 25, 'num_indisc_conc': 36, 'num_targ_repr': 24, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, 'final_pop': 255, 'total_num_attacks': 824, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6551724137931034, 0.6071428571428571, 0.6666666666666666], [0.7241379310344828, 0.4, 0.7857142857142857], [0.6956521739130435, 0.6060606060606061, 0.5555555555555556]], 'num_targ_conc': 21, 'num_indisc_conc': 14, 'num_targ_repr': 32, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, 'final_pop': 768, 'total_num_attacks': 1946, 'dominant_sentiments': [['neutral', 'anti-violence', 'neutral', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence', 'neutral'], ['neutral', 'anti-violence', 'anti-violence', 'neutral', 'sympathetic'], ['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5, 0.75, 0.4722222222222222, 0.6363636363636364, 0.8076923076923077], [0.9032258064516129, 0.5, 0.8214285714285714, 0.6666666666666666, 0.9545454545454546], [0.8518518518518519, 0.5757575757575758, 0.5, 0.72, 0.4117647058823529], [0.36666666666666664, 0.875, 0.34146341463414637, 0.46875, 0.59375], [0.6666666666666666, 0.4594594594594595, 0.8181818181818182, 0.9333333333333333, 0.45714285714285713]], 'num_targ_conc': 13, 'num_indisc_conc': 14, 'num_targ_repr': 17, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 220, 'total_num_attacks': 541, 'dominant_sentiments': [['neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.37142857142857144, 0.5, 0.5], [0.5142857142857142, 0.5531914893617021, 0.3783783783783784]], 'num_targ_conc': 20, 'num_indisc_conc': 22, 'num_targ_repr': 20, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 285, 'total_num_attacks': 1253, 'dominant_sentiments': [['anti-violence', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral'], ['neutral', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.36363636363636365, 0.4375, 0.3939393939393939], [0.42857142857142855, 0.40625, 0.3793103448275862], [0.4230769230769231, 0.35294117647058826, 0.6551724137931034]], 'num_targ_conc': 42, 'num_indisc_conc': 22, 'num_targ_repr': 26, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.003, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, 'final_pop': 511, 'total_num_attacks': 1903, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6774193548387096, 0.8333333333333334, 0.9523809523809523, 0.8461538461538461, 0.88], [0.6896551724137931, 0.8260869565217391, 0.9310344827586207, 0.92, 0.9166666666666666], [0.8387096774193549, 0.875, 0.9565217391304348, 0.92, 0.76], [0.8333333333333334, 0.5625, 0.8620689655172413, 0.8148148148148148, 0.875]], 'num_targ_conc': 21, 'num_indisc_conc': 21, 'num_targ_repr': 20, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 215, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 85, 'num_indisc_conc': 83, 'num_targ_repr': 35, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 419, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 27, 'num_indisc_conc': 40, 'num_targ_repr': 19, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 764, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 25, 'num_indisc_conc': 29, 'num_targ_repr': 28, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 800, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 892, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 31, 'num_indisc_conc': 25, 'num_targ_repr': 17, 'num_indisc_repr': 12}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 800, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 933, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 95, 'num_indisc_conc': 82, 'num_targ_repr': 31, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 872, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 43, 'num_indisc_conc': 48, 'num_targ_repr': 21, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 800, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 829, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 115, 'num_indisc_conc': 128, 'num_targ_repr': 35, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 900, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 72, 'num_indisc_conc': 68, 'num_targ_repr': 37, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 700}, 'final_pop': 1, 'total_num_attacks': 343, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 141, 'num_indisc_conc': 134, 'num_targ_repr': 35, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 2, 'total_num_attacks': 270, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 1.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 126, 'num_indisc_conc': 125, 'num_targ_repr': 43, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, 'final_pop': 6, 'total_num_attacks': 318, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0], [0.0, 1.0, 1.0], [1.0, 1.0, 1.0]], 'num_targ_conc': 124, 'num_indisc_conc': 130, 'num_targ_repr': 30, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 828, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 54, 'num_indisc_conc': 51, 'num_targ_repr': 33, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500}, 'final_pop': 38, 'total_num_attacks': 2053, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.4, 1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 0.6666666666666666, 1.0, 0.0], [0.6666666666666666, 1.0, 1.0, 0.0, 1.0], [1.0, 0.5, 0.0, 1.0, 0.0]], 'num_targ_conc': 73, 'num_indisc_conc': 56, 'num_targ_repr': 85, 'num_indisc_repr': 83}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 878, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 47, 'num_indisc_conc': 46, 'num_targ_repr': 40, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 416, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 95, 'num_indisc_conc': 92, 'num_targ_repr': 37, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 548, 'total_num_attacks': 1021, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8409090909090909, 0.6909090909090909, 0.8372093023255814, 0.7321428571428571], [0.7804878048780488, 0.7755102040816326, 0.7368421052631579, 0.7454545454545455], [0.7631578947368421, 0.8695652173913043, 0.8484848484848485, 0.8478260869565217]], 'num_targ_conc': 39, 'num_indisc_conc': 36, 'num_targ_repr': 10, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 300}, 'final_pop': 499, 'total_num_attacks': 1605, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7608695652173914, 0.9210526315789473, 0.7551020408163265, 0.7631578947368421], [0.7837837837837838, 0.7727272727272727, 0.6857142857142857, 0.8536585365853658], [0.8222222222222222, 0.8260869565217391, 0.8163265306122449, 0.7894736842105263]], 'num_targ_conc': 49, 'num_indisc_conc': 46, 'num_targ_repr': 11, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.003, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 1788, 'total_num_attacks': 1271, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9664429530201343, 0.9591836734693877, 0.9805194805194806, 0.9810126582278481], [0.9545454545454546, 0.9426751592356688, 0.9717514124293786, 0.9452054794520548], [0.9453125, 0.950920245398773, 0.9395973154362416, 0.9398496240601504]], 'num_targ_conc': 38, 'num_indisc_conc': 37, 'num_targ_repr': 8, 'num_indisc_repr': 10}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 177, 'total_num_attacks': 519, 'dominant_sentiments': [['anti-violence', 'neutral', 'sympathetic'], ['anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4, 0.5483870967741935, 0.5], [0.7931034482758621, 0.4827586206896552, 0.4375]], 'num_targ_conc': 7, 'num_indisc_conc': 4, 'num_targ_repr': 28, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 157, 'total_num_attacks': 1206, 'dominant_sentiments': [['neutral', 'sympathetic', 'anti-violence', 'anti-violence'], ['sympathetic', 'neutral', 'neutral', 'sympathetic'], ['sympathetic', 'sympathetic', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.625, 0.5, 0.5625, 0.5], [0.4375, 0.4666666666666667, 0.7, 0.5454545454545454], [0.4, 0.4666666666666667, 0.4166666666666667, 0.5]], 'num_targ_conc': 19, 'num_indisc_conc': 8, 'num_targ_repr': 44, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.003, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 89, 'total_num_attacks': 3674, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'anti-violence', 'neutral', 'neutral'], ['anti-violence', 'neutral', 'neutral', 'anti-violence', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'neutral', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.5, 1.0, 0.6666666666666666], [0.5, 0.6666666666666666, 0.5, 0.6666666666666666, 0.6666666666666666], [0.6666666666666666, 0.5, 0.5, 1.0, 0.6666666666666666], [0.75, 0.5714285714285714, 0.6, 0.75, 1.0], [0.5, 0.6666666666666666, 0.5, 0.4, 0.5]], 'num_targ_conc': 13, 'num_indisc_conc': 15, 'num_targ_repr': 52, 'num_indisc_repr': 52}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 218, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 10, 'num_indisc_conc': 22, 'num_targ_repr': 35, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 688, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 26, 'num_indisc_conc': 29, 'num_targ_repr': 39, 'num_indisc_repr': 44}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 1, 'total_num_attacks': 679, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]], 'num_targ_conc': 16, 'num_indisc_conc': 20, 'num_targ_repr': 31, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 294, 'total_num_attacks': 755, 'dominant_sentiments': [['neutral', 'sympathetic', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4375, 0.3870967741935484, 0.8823529411764706], [0.3939393939393939, 0.3448275862068966, 0.40625], [0.6, 0.65625, 0.65625]], 'num_targ_conc': 22, 'num_indisc_conc': 14, 'num_targ_repr': 17, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, 'final_pop': 286, 'total_num_attacks': 1852, 'dominant_sentiments': [['neutral', 'sympathetic', 'anti-violence'], ['anti-violence', 'sympathetic', 'anti-violence'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4666666666666667, 0.5, 0.7741935483870968], [0.45714285714285713, 0.48484848484848486, 0.38235294117647056], [0.41379310344827586, 0.45454545454545453, 0.41379310344827586]], 'num_targ_conc': 39, 'num_indisc_conc': 50, 'num_targ_repr': 69, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 500}, 'final_pop': 369, 'total_num_attacks': 2539, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'sympathetic', 'anti-violence']], 'percent_dominant_sentiments': [[0.6388888888888888, 0.7142857142857143, 0.5454545454545454, 0.9615384615384616], [0.5142857142857142, 0.75, 0.6470588235294118, 0.9411764705882353], [0.37142857142857144, 0.45714285714285713, 0.38235294117647056, 0.8571428571428571]], 'num_targ_conc': 39, 'num_indisc_conc': 51, 'num_targ_repr': 53, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, 'final_pop': 372, 'total_num_attacks': 3336, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9, 0.59375, 0.6451612903225806, 0.5517241379310345], [0.4117647058823529, 0.9259259259259259, 0.9, 0.5], [0.45454545454545453, 0.4166666666666667, 0.7666666666666667, 0.46875]], 'num_targ_conc': 51, 'num_indisc_conc': 63, 'num_targ_repr': 68, 'num_indisc_repr': 70}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 767, 'total_num_attacks': 2080, 'dominant_sentiments': [['neutral', 'anti-violence', 'anti-violence', 'sympathetic', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.3939393939393939, 0.7241379310344828, 0.6363636363636364, 0.53125, 0.6923076923076923], [0.40625, 0.5757575757575758, 0.5625, 0.5862068965517241, 0.6071428571428571], [0.5454545454545454, 0.41379310344827586, 0.5, 0.43333333333333335, 0.45161290322580644], [0.6333333333333333, 0.6388888888888888, 0.6551724137931034, 0.6363636363636364, 0.8333333333333334], [0.75, 0.5172413793103449, 0.3548387096774194, 0.5714285714285714, 0.48148148148148145]], 'num_targ_conc': 20, 'num_indisc_conc': 16, 'num_targ_repr': 12, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 180, 'total_num_attacks': 2092, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.46875, 0.43333333333333335, 0.5], [0.5294117647058824, 0.4482758620689655, 0.48]], 'num_targ_conc': 56, 'num_indisc_conc': 70, 'num_targ_repr': 62, 'num_indisc_repr': 71}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, 'final_pop': 284, 'total_num_attacks': 2160, 'dominant_sentiments': [['neutral', 'neutral', 'neutral'], ['sympathetic', 'sympathetic', 'neutral'], ['neutral', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.5625, 0.4838709677419355, 0.5], [0.5806451612903226, 0.4117647058823529, 0.3870967741935484], [0.47058823529411764, 0.45161290322580644, 0.3548387096774194]], 'num_targ_conc': 47, 'num_indisc_conc': 40, 'num_targ_repr': 54, 'num_indisc_repr': 58}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, 'final_pop': 413, 'total_num_attacks': 1527, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7352941176470589, 0.78125, 0.575, 0.8666666666666667], [0.6176470588235294, 0.7428571428571429, 0.6410256410256411, 0.7666666666666667], [0.84375, 0.78125, 0.7272727272727273, 0.6521739130434783]], 'num_targ_conc': 37, 'num_indisc_conc': 30, 'num_targ_repr': 23, 'num_indisc_repr': 33}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, 'final_pop': 437, 'total_num_attacks': 2930, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'neutral', 'sympathetic'], ['sympathetic', 'sympathetic', 'sympathetic', 'sympathetic'], ['sympathetic', 'neutral', 'sympathetic', 'sympathetic']], 'percent_dominant_sentiments': [[0.4, 0.48717948717948717, 0.45161290322580644, 0.36363636363636365], [0.5, 0.6060606060606061, 0.5161290322580645, 0.4666666666666667], [0.5263157894736842, 0.3684210526315789, 0.4375, 0.45]], 'num_targ_conc': 49, 'num_indisc_conc': 40, 'num_targ_repr': 49, 'num_indisc_repr': 49}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, 'final_pop': 696, 'total_num_attacks': 2668, 'dominant_sentiments': [['neutral', 'sympathetic', 'sympathetic', 'sympathetic', 'sympathetic'], ['neutral', 'neutral', 'neutral', 'neutral', 'sympathetic'], ['sympathetic', 'neutral', 'sympathetic', 'sympathetic', 'sympathetic'], ['sympathetic', 'sympathetic', 'sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4722222222222222, 0.59375, 0.4444444444444444, 0.47368421052631576, 0.6470588235294118], [0.42424242424242425, 0.45161290322580644, 0.5, 0.48717948717948717, 0.5121951219512195], [0.5454545454545454, 0.41935483870967744, 0.7272727272727273, 0.4473684210526316, 0.45714285714285713], [0.5588235294117647, 0.5428571428571428, 0.5588235294117647, 0.4864864864864865, 0.5]], 'num_targ_conc': 34, 'num_indisc_conc': 29, 'num_targ_repr': 32, 'num_indisc_repr': 26}, {'params': {'prob_violence': 0.005, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 289, 'total_num_attacks': 1288, 'dominant_sentiments': [['neutral', 'anti-violence', 'neutral'], ['neutral', 'neutral', 'neutral'], ['neutral', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.4878048780487805, 0.4375, 0.46875], [0.4827586206896552, 0.43333333333333335, 0.41379310344827586], [0.38235294117647056, 0.5172413793103449, 0.46875]], 'num_targ_conc': 36, 'num_indisc_conc': 33, 'num_targ_repr': 29, 'num_indisc_repr': 24}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 800, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 926, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 116, 'num_indisc_conc': 108, 'num_targ_repr': 56, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 317, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 48, 'num_indisc_conc': 48, 'num_targ_repr': 20, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 480, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 55, 'num_indisc_conc': 55, 'num_targ_repr': 23, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 876, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 30, 'num_targ_repr': 10, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 819, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 49, 'num_indisc_conc': 53, 'num_targ_repr': 14, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 443, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 30, 'num_targ_repr': 15, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 700}, 'final_pop': 14, 'total_num_attacks': 771, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 125, 'num_indisc_conc': 121, 'num_targ_repr': 42, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, 'final_pop': 12, 'total_num_attacks': 661, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 1.0, 1.0], [0.0, 0.5, 1.0], [1.0, 0.0, 1.0]], 'num_targ_conc': 41, 'num_indisc_conc': 50, 'num_targ_repr': 29, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, 'final_pop': 11, 'total_num_attacks': 2150, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 95, 'num_indisc_conc': 96, 'num_targ_repr': 124, 'num_indisc_repr': 126}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 259, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 31, 'num_targ_repr': 16, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 762, 'total_num_attacks': 1196, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9512195121951219, 0.9846153846153847, 0.9558823529411765, 1.0], [0.96875, 1.0, 0.9696969696969697, 0.9818181818181818], [0.9852941176470589, 0.9846153846153847, 1.0, 1.0]], 'num_targ_conc': 39, 'num_indisc_conc': 36, 'num_targ_repr': 4, 'num_indisc_repr': 8}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 200}, 'final_pop': 1207, 'total_num_attacks': 975, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5736434108527132, 0.6013986013986014, 0.5757575757575758], [0.632, 0.5564516129032258, 0.5725190839694656], [0.42657342657342656, 0.6159420289855072, 0.5704225352112676]], 'num_targ_conc': 31, 'num_indisc_conc': 34, 'num_targ_repr': 8, 'num_indisc_repr': 14}, {'params': {'prob_violence': 0.005, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 300}, 'final_pop': 432, 'total_num_attacks': 2402, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'anti-violence'], ['sympathetic', 'neutral', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.34375, 0.3953488372093023, 0.4878048780487805, 0.42105263157894735], [0.36585365853658536, 0.36363636363636365, 0.4722222222222222, 0.375], [0.4166666666666667, 0.40540540540540543, 0.41379310344827586, 0.3888888888888889]], 'num_targ_conc': 55, 'num_indisc_conc': 53, 'num_targ_repr': 9, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 1143, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 25, 'num_indisc_conc': 34, 'num_targ_repr': 99, 'num_indisc_repr': 88}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 475, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 8, 'num_indisc_conc': 14, 'num_targ_repr': 42, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 193, 'total_num_attacks': 538, 'dominant_sentiments': [['neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.4, 0.4, 0.3333333333333333], [0.5161290322580645, 0.4, 0.40625]], 'num_targ_conc': 12, 'num_indisc_conc': 5, 'num_targ_repr': 25, 'num_indisc_repr': 42}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 25, 'total_num_attacks': 1381, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.36363636363636365, 1.0, 0.5], [0.75, 0.5, 0.75]], 'num_targ_conc': 33, 'num_indisc_conc': 31, 'num_targ_repr': 131, 'num_indisc_repr': 127}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 627, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 12, 'num_indisc_conc': 8, 'num_targ_repr': 56, 'num_indisc_repr': 56}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 589, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 24, 'num_indisc_conc': 31, 'num_targ_repr': 136, 'num_indisc_repr': 126}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 331, 'total_num_attacks': 937, 'dominant_sentiments': [['neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.4838709677419355, 0.47619047619047616, 0.5, 0.6296296296296297], [0.41379310344827586, 0.4, 0.39285714285714285, 0.5555555555555556], [0.5555555555555556, 0.5185185185185185, 0.40625, 0.4482758620689655]], 'num_targ_conc': 7, 'num_indisc_conc': 7, 'num_targ_repr': 37, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.005, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, 'final_pop': 171, 'total_num_attacks': 1418, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.8333333333333334, 0.6666666666666666, 0.5, 0.75], [0.75, 1.0, 0.875, 1.0, 0.8888888888888888], [0.6666666666666666, 0.5, 0.7272727272727273, 0.875, 0.9333333333333333], [0.8, 1.0, 0.8, 0.8333333333333334, 0.75], [1.0, 1.0, 0.875, 0.6, 0.7777777777777778]], 'num_targ_conc': 10, 'num_indisc_conc': 6, 'num_targ_repr': 38, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 700}, 'final_pop': 149, 'total_num_attacks': 2483, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.95, 0.9285714285714286, 1.0], [1.0, 0.9473684210526315, 0.9523809523809523], [0.9090909090909091, 0.9333333333333333, 0.9583333333333334]], 'num_targ_conc': 69, 'num_indisc_conc': 76, 'num_targ_repr': 75, 'num_indisc_repr': 65}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 81, 'total_num_attacks': 647, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral'], ['anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.6470588235294118, 0.5294117647058824, 0.45454545454545453], [0.4, 0.4444444444444444, 0.5833333333333334]], 'num_targ_conc': 19, 'num_indisc_conc': 14, 'num_targ_repr': 23, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 300, 'steps': 700}, 'final_pop': 289, 'total_num_attacks': 2943, 'dominant_sentiments': [['neutral', 'neutral', 'neutral'], ['neutral', 'sympathetic', 'sympathetic'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.41935483870967744, 0.3888888888888889, 0.5161290322580645], [0.6176470588235294, 0.53125, 0.5666666666666667], [0.4482758620689655, 0.48484848484848486, 0.47058823529411764]], 'num_targ_conc': 84, 'num_indisc_conc': 70, 'num_targ_repr': 64, 'num_indisc_repr': 63}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, 'final_pop': 375, 'total_num_attacks': 1635, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'sympathetic', 'sympathetic', 'neutral']], 'percent_dominant_sentiments': [[0.4, 0.4666666666666667, 0.4666666666666667, 0.4117647058823529], [0.48484848484848486, 0.4375, 0.4, 0.5625], [0.35294117647058826, 0.5151515151515151, 0.4827586206896552, 0.5517241379310345]], 'num_targ_conc': 36, 'num_indisc_conc': 22, 'num_targ_repr': 28, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.008, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 500}, 'final_pop': 196, 'total_num_attacks': 1380, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.90625, 0.8823529411764706, 0.9411764705882353], [1.0, 0.9629629629629629, 1.0]], 'num_targ_conc': 50, 'num_indisc_conc': 48, 'num_targ_repr': 45, 'num_indisc_repr': 48}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 428, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 76, 'num_indisc_conc': 87, 'num_targ_repr': 37, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 420, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 28, 'num_indisc_conc': 28, 'num_targ_repr': 6, 'num_indisc_repr': 14}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 482, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 31, 'num_indisc_conc': 34, 'num_targ_repr': 18, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'high', 'starting_population': 600, 'steps': 700}, 'final_pop': 1, 'total_num_attacks': 749, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 126, 'num_indisc_conc': 105, 'num_targ_repr': 59, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 372, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 42, 'num_indisc_conc': 36, 'num_targ_repr': 15, 'num_indisc_repr': 30}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 400, 'steps': 200}, 'final_pop': 14, 'total_num_attacks': 410, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.5], [0.0, 1.0, 1.0, 0.0]], 'num_targ_conc': 36, 'num_indisc_conc': 36, 'num_targ_repr': 9, 'num_indisc_repr': 13}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 1019, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 28, 'num_targ_repr': 16, 'num_indisc_repr': 27}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 14, 'total_num_attacks': 210, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8, 1.0, 1.0], [1.0, 1.0, 1.0]], 'num_targ_conc': 43, 'num_indisc_conc': 44, 'num_targ_repr': 14, 'num_indisc_repr': 10}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, 'final_pop': 7, 'total_num_attacks': 464, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 1.0, 1.0], [1.0, 1.0, 1.0]], 'num_targ_conc': 42, 'num_indisc_conc': 56, 'num_targ_repr': 32, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 8, 'total_num_attacks': 918, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6666666666666666, 1.0, 1.0], [1.0, 1.0, 1.0]], 'num_targ_conc': 109, 'num_indisc_conc': 101, 'num_targ_repr': 81, 'num_indisc_repr': 107}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 300}, 'final_pop': 2, 'total_num_attacks': 390, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 0.0, 0.0]], 'num_targ_conc': 38, 'num_indisc_conc': 60, 'num_targ_repr': 14, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 2, 'total_num_attacks': 857, 'dominant_sentiments': [['NONE', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], 'num_targ_conc': 30, 'num_indisc_conc': 32, 'num_targ_repr': 28, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 400, 'steps': 500}, 'final_pop': 2, 'total_num_attacks': 1306, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]], 'num_targ_conc': 83, 'num_indisc_conc': 81, 'num_targ_repr': 79, 'num_indisc_repr': 57}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, 'final_pop': 408, 'total_num_attacks': 1503, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.5789473684210527, 0.68, 0.3888888888888889, 0.6818181818181818], [0.7142857142857143, 0.4583333333333333, 0.8823529411764706, 0.5454545454545454, 0.47368421052631576], [0.6086956521739131, 0.7083333333333334, 0.56, 0.6428571428571429, 0.5714285714285714], [0.5333333333333333, 0.5714285714285714, 0.7727272727272727, 0.38095238095238093, 0.4090909090909091]], 'num_targ_conc': 70, 'num_indisc_conc': 73, 'num_targ_repr': 89, 'num_indisc_repr': 96}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 1235, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 129, 'num_indisc_conc': 119, 'num_targ_repr': 55, 'num_indisc_repr': 59}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 800, 'steps': 500}, 'final_pop': 13, 'total_num_attacks': 1706, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'sympathetic', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'sympathetic', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.6666666666666666], [0.0, 0.0, 1.0, 0.5, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 0.0]], 'num_targ_conc': 58, 'num_indisc_conc': 80, 'num_targ_repr': 79, 'num_indisc_repr': 80}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 231, 'total_num_attacks': 1096, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8076923076923077, 0.9259259259259259, 0.8888888888888888], [0.9565217391304348, 0.92, 0.84375], [0.9285714285714286, 0.8571428571428571, 1.0]], 'num_targ_conc': 45, 'num_indisc_conc': 36, 'num_targ_repr': 62, 'num_indisc_repr': 78}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 700}, 'final_pop': 1, 'total_num_attacks': 443, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 120, 'num_indisc_conc': 123, 'num_targ_repr': 38, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, 'final_pop': 318, 'total_num_attacks': 1419, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['sympathetic', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.3870967741935484, 0.39473684210526316, 0.5121951219512195], [0.43333333333333335, 0.35294117647058826, 0.5483870967741935], [0.6052631578947368, 0.5555555555555556, 0.6060606060606061]], 'num_targ_conc': 55, 'num_indisc_conc': 48, 'num_targ_repr': 10, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 751, 'total_num_attacks': 1552, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7368421052631579, 0.4375, 0.7428571428571429, 0.3939393939393939, 0.71875], [0.7857142857142857, 0.5897435897435898, 0.6153846153846154, 0.625, 0.6666666666666666], [0.6666666666666666, 0.775, 0.5681818181818182, 0.7073170731707317, 0.7631578947368421], [0.6756756756756757, 0.5428571428571428, 0.6666666666666666, 0.5666666666666667, 0.7567567567567568]], 'num_targ_conc': 40, 'num_indisc_conc': 26, 'num_targ_repr': 10, 'num_indisc_repr': 7}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 333, 'total_num_attacks': 587, 'dominant_sentiments': [['sympathetic', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.3829787234042553, 0.417910447761194, 0.3220338983050847], [0.42857142857142855, 0.3829787234042553, 0.45588235294117646]], 'num_targ_conc': 32, 'num_indisc_conc': 37, 'num_targ_repr': 8, 'num_indisc_repr': 6}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 436, 'total_num_attacks': 562, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8059701492537313, 0.8450704225352113, 0.8695652173913043], [0.835820895522388, 0.875, 0.8153846153846154]], 'num_targ_conc': 39, 'num_indisc_conc': 36, 'num_targ_repr': 6, 'num_indisc_repr': 8}, {'params': {'prob_violence': 0.008, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 2801, 'total_num_attacks': 2029, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.910828025477707, 0.8904109589041096, 0.8662207357859532], [0.9076923076923077, 0.9303030303030303, 0.909375], [0.8601823708206687, 0.8511326860841424, 0.8600583090379009]], 'num_targ_conc': 58, 'num_indisc_conc': 60, 'num_targ_repr': 14, 'num_indisc_repr': 7}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 266, 'total_num_attacks': 722, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6875, 0.8, 0.4482758620689655], [0.4642857142857143, 0.9259259259259259, 0.6], [0.5428571428571428, 0.8275862068965517, 0.42424242424242425]], 'num_targ_conc': 13, 'num_indisc_conc': 11, 'num_targ_repr': 30, 'num_indisc_repr': 35}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 275, 'total_num_attacks': 1565, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6, 0.6923076923076923, 0.75, 0.5833333333333334, 0.75], [1.0, 0.7, 0.7368421052631579, 0.4666666666666667, 0.6], [0.6153846153846154, 0.7777777777777778, 0.5333333333333333, 0.625, 0.8888888888888888], [0.7, 0.7, 0.6875, 0.6, 0.7222222222222222]], 'num_targ_conc': 6, 'num_indisc_conc': 6, 'num_targ_repr': 41, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 300}, 'final_pop': 399, 'total_num_attacks': 1235, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'sympathetic', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.3611111111111111, 0.40625, 0.36666666666666664, 0.42424242424242425], [0.4857142857142857, 0.4838709677419355, 0.4375, 0.37142857142857144], [0.37142857142857144, 0.5128205128205128, 0.4375, 0.42857142857142855]], 'num_targ_conc': 9, 'num_indisc_conc': 17, 'num_targ_repr': 55, 'num_indisc_repr': 45}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 2124, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 16, 'num_indisc_conc': 17, 'num_targ_repr': 103, 'num_indisc_repr': 95}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 326, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 24, 'num_indisc_conc': 9, 'num_targ_repr': 54, 'num_indisc_repr': 52}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 1052, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 7, 'num_indisc_conc': 17, 'num_targ_repr': 46, 'num_indisc_repr': 58}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 626, 'total_num_attacks': 3364, 'dominant_sentiments': [['neutral', 'sympathetic', 'neutral', 'anti-violence', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral', 'anti-violence'], ['neutral', 'anti-violence', 'neutral', 'neutral', 'neutral'], ['sympathetic', 'neutral', 'anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.48148148148148145, 0.4090909090909091, 0.4782608695652174, 0.4230769230769231, 0.5454545454545454], [0.47368421052631576, 0.4482758620689655, 0.375, 0.37037037037037035, 0.46153846153846156], [0.5, 0.375, 0.38461538461538464, 0.5217391304347826, 0.45161290322580644], [0.375, 0.4666666666666667, 0.5483870967741935, 0.5384615384615384, 0.56], [0.4583333333333333, 0.5416666666666666, 0.3448275862068966, 0.37037037037037035, 0.4827586206896552]], 'num_targ_conc': 14, 'num_indisc_conc': 15, 'num_targ_repr': 61, 'num_indisc_repr': 58}, {'params': {'prob_violence': 0.008, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 3620, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 22, 'num_indisc_conc': 18, 'num_targ_repr': 82, 'num_indisc_repr': 86}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 415, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 18, 'num_indisc_conc': 20, 'num_targ_repr': 15, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 719, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 21, 'num_indisc_conc': 17, 'num_targ_repr': 30, 'num_indisc_repr': 32}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 5, 'total_num_attacks': 415, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral'], ['anti-violence', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.5, 1.0, 1.0], [0.0, 1.0, 1.0]], 'num_targ_conc': 35, 'num_indisc_conc': 33, 'num_targ_repr': 43, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 400, 'steps': 700}, 'final_pop': 365, 'total_num_attacks': 3639, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'sympathetic', 'neutral'], ['neutral', 'sympathetic', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.9259259259259259, 0.8125, 0.6764705882352942, 0.7575757575757576], [0.8333333333333334, 0.9629629629629629, 0.5151515151515151, 0.41935483870967744], [0.65625, 0.5357142857142857, 0.9375, 0.42424242424242425]], 'num_targ_conc': 46, 'num_indisc_conc': 62, 'num_targ_repr': 69, 'num_indisc_repr': 84}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 300}, 'final_pop': 196, 'total_num_attacks': 858, 'dominant_sentiments': [['neutral', 'sympathetic', 'sympathetic'], ['sympathetic', 'sympathetic', 'sympathetic']], 'percent_dominant_sentiments': [[0.4838709677419355, 0.5806451612903226, 0.4444444444444444], [0.375, 0.5151515151515151, 0.46875]], 'num_targ_conc': 30, 'num_indisc_conc': 22, 'num_targ_repr': 36, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 240, 'total_num_attacks': 2086, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral', 'neutral', 'neutral'], ['neutral', 'neutral', 'neutral', 'anti-violence', 'neutral'], ['sympathetic', 'neutral', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'sympathetic', 'sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.3, 0.38461538461538464, 0.6470588235294118, 0.5833333333333334, 0.5], [0.4, 0.4444444444444444, 0.4375, 0.5, 0.375], [0.5454545454545454, 0.5, 0.5294117647058824, 0.6, 0.7272727272727273], [0.46153846153846156, 0.5, 0.47368421052631576, 0.4, 0.5714285714285714]], 'num_targ_conc': 17, 'num_indisc_conc': 18, 'num_targ_repr': 18, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 183, 'total_num_attacks': 603, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic'], ['sympathetic', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5666666666666667, 0.5161290322580645, 0.46875], [0.43333333333333335, 0.4375, 0.4375]], 'num_targ_conc': 18, 'num_indisc_conc': 31, 'num_targ_repr': 21, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 700}, 'final_pop': 187, 'total_num_attacks': 2099, 'dominant_sentiments': [['anti-violence', 'neutral', 'neutral'], ['sympathetic', 'neutral', 'neutral']], 'percent_dominant_sentiments': [[0.3684210526315789, 0.5, 0.4375], [0.41379310344827586, 0.5666666666666667, 0.4]], 'num_targ_conc': 79, 'num_indisc_conc': 63, 'num_targ_repr': 67, 'num_indisc_repr': 66}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'NONE', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 500}, 'final_pop': 271, 'total_num_attacks': 2050, 'dominant_sentiments': [['anti-violence', 'sympathetic', 'neutral'], ['sympathetic', 'neutral', 'neutral'], ['neutral', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.4, 0.4838709677419355, 0.6551724137931034], [0.42857142857142855, 0.5333333333333333, 0.3939393939393939], [0.43333333333333335, 0.46875, 0.4827586206896552]], 'num_targ_conc': 47, 'num_indisc_conc': 47, 'num_targ_repr': 43, 'num_indisc_repr': 56}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 3, 'total_num_attacks': 611, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 33, 'num_targ_repr': 18, 'num_indisc_repr': 12}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 830, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 34, 'num_indisc_conc': 32, 'num_targ_repr': 21, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'high', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 224, 'dominant_sentiments': [['INDISC-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 55, 'num_indisc_conc': 50, 'num_targ_repr': 23, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 313, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 33, 'num_targ_repr': 17, 'num_indisc_repr': 12}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'high', 'starting_population': 600, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 629, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 84, 'num_indisc_conc': 91, 'num_targ_repr': 32, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 200, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 219, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 115, 'num_indisc_conc': 114, 'num_targ_repr': 36, 'num_indisc_repr': 34}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 300, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 325, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 78, 'num_indisc_conc': 90, 'num_targ_repr': 39, 'num_indisc_repr': 40}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-high', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 449, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 41, 'num_indisc_conc': 30, 'num_targ_repr': 13, 'num_indisc_repr': 18}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'mid', 'starting_population': 600, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 754, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 97, 'num_indisc_conc': 116, 'num_targ_repr': 50, 'num_indisc_repr': 51}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 3, 'total_num_attacks': 424, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 29, 'num_indisc_conc': 34, 'num_targ_repr': 22, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'mid-low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 823, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 48, 'num_indisc_conc': 54, 'num_targ_repr': 17, 'num_indisc_repr': 15}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 457, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 32, 'num_indisc_conc': 27, 'num_targ_repr': 21, 'num_indisc_repr': 25}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, 'final_pop': 3, 'total_num_attacks': 476, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5, 0.0, 1.0], [1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 85, 'num_indisc_conc': 78, 'num_targ_repr': 35, 'num_indisc_repr': 36}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 4, 'total_num_attacks': 335, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 1.0, 0.0], [1.0, 1.0, 0.0]], 'num_targ_conc': 35, 'num_indisc_conc': 36, 'num_targ_repr': 21, 'num_indisc_repr': 19}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 485, 'dominant_sentiments': [['TARG-CONC', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 30, 'num_indisc_conc': 33, 'num_targ_repr': 15, 'num_indisc_repr': 12}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'low', 'discontent': 'low', 'starting_population': 800, 'steps': 300}, 'final_pop': 454, 'total_num_attacks': 1434, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7894736842105263, 0.7916666666666666, 0.6666666666666666, 0.7333333333333333, 0.8181818181818182], [0.9333333333333333, 0.7692307692307693, 0.7777777777777778, 0.7142857142857143, 0.6666666666666666], [0.8235294117647058, 0.5454545454545454, 0.6956521739130435, 0.8235294117647058, 0.8181818181818182], [0.7857142857142857, 0.7222222222222222, 0.6521739130434783, 1.0, 0.8181818181818182], [0.6363636363636364, 0.782608695652174, 0.9230769230769231, 0.7333333333333333, 0.7368421052631579]], 'num_targ_conc': 45, 'num_indisc_conc': 36, 'num_targ_repr': 50, 'num_indisc_repr': 56}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, 'final_pop': 642, 'total_num_attacks': 2362, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7156862745098039, 0.7, 0.6224489795918368], [0.6410256410256411, 0.6814159292035398, 0.7663551401869159]], 'num_targ_conc': 97, 'num_indisc_conc': 95, 'num_targ_repr': 13, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 700}, 'final_pop': 594, 'total_num_attacks': 2832, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.89, 0.8958333333333334, 0.8936170212765957], [0.9204545454545454, 0.9081632653061225, 0.9026548672566371]], 'num_targ_conc': 137, 'num_indisc_conc': 130, 'num_targ_repr': 28, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 296, 'total_num_attacks': 915, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.9310344827586207, 0.8787878787878788, 0.90625], [1.0, 0.8787878787878788, 0.9705882352941176], [0.8888888888888888, 0.8611111111111112, 0.8285714285714286]], 'num_targ_conc': 48, 'num_indisc_conc': 36, 'num_targ_repr': 11, 'num_indisc_repr': 7}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 300}, 'final_pop': 417, 'total_num_attacks': 1430, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7380952380952381, 0.7297297297297297, 0.7674418604651163], [0.8, 0.8780487804878049, 0.6909090909090909], [0.8085106382978723, 0.74, 0.7391304347826086]], 'num_targ_conc': 49, 'num_indisc_conc': 48, 'num_targ_repr': 9, 'num_indisc_repr': 16}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, 'final_pop': 267, 'total_num_attacks': 2711, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.896551724137931, 0.68, 0.5862068965517241], [0.6666666666666666, 0.7222222222222222, 0.6571428571428571], [0.6206896551724138, 0.6451612903225806, 0.5625]], 'num_targ_conc': 93, 'num_indisc_conc': 79, 'num_targ_repr': 14, 'num_indisc_repr': 23}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 200}, 'final_pop': 755, 'total_num_attacks': 1655, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6046511627906976, 0.8235294117647058, 0.7878787878787878, 0.6944444444444444, 0.6666666666666666], [0.7714285714285715, 0.6923076923076923, 0.675, 0.8823529411764706, 0.7575757575757576], [0.7435897435897436, 0.7, 0.7777777777777778, 0.6842105263157895, 0.8], [0.6923076923076923, 0.6, 0.6511627906976745, 0.696969696969697, 0.75]], 'num_targ_conc': 42, 'num_indisc_conc': 33, 'num_targ_repr': 9, 'num_indisc_repr': 12}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 300}, 'final_pop': 716, 'total_num_attacks': 2597, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7428571428571429, 0.7142857142857143, 0.8709677419354839, 0.7567567567567568, 0.7692307692307693], [0.8857142857142857, 0.8648648648648649, 0.7428571428571429, 0.7878787878787878, 0.7142857142857143], [0.7741935483870968, 0.7647058823529411, 0.8484848484848485, 0.5238095238095238, 0.8125], [0.7380952380952381, 0.6111111111111112, 0.7714285714285715, 0.7777777777777778, 0.8157894736842105]], 'num_targ_conc': 56, 'num_indisc_conc': 53, 'num_targ_repr': 13, 'num_indisc_repr': 21}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 300}, 'final_pop': 793, 'total_num_attacks': 3505, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.5128205128205128, 0.4375, 0.6896551724137931, 0.5666666666666667, 0.6896551724137931], [0.34375, 0.9666666666666667, 0.7878787878787878, 0.4375, 0.7272727272727273], [0.7333333333333333, 0.7241379310344828, 0.8666666666666667, 0.6333333333333333, 0.9666666666666667], [0.7878787878787878, 0.8, 1.0, 0.8181818181818182, 0.6388888888888888], [0.7333333333333333, 0.7352941176470589, 0.7647058823529411, 0.6764705882352942, 0.7222222222222222]], 'num_targ_conc': 55, 'num_indisc_conc': 53, 'num_targ_repr': 9, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 400, 'steps': 200}, 'final_pop': 787, 'total_num_attacks': 1001, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6153846153846154, 0.5373134328358209, 0.578125, 0.6428571428571429], [0.6612903225806451, 0.6285714285714286, 0.6285714285714286, 0.4861111111111111], [0.6451612903225806, 0.671875, 0.6785714285714286, 0.6212121212121212]], 'num_targ_conc': 35, 'num_indisc_conc': 32, 'num_targ_repr': 10, 'num_indisc_repr': 20}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 800, 'steps': 300}, 'final_pop': 1804, 'total_num_attacks': 4441, 'dominant_sentiments': [['sympathetic', 'neutral', 'neutral', 'neutral', 'neutral'], ['sympathetic', 'sympathetic', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'neutral', 'neutral', 'anti-violence', 'sympathetic'], ['neutral', 'sympathetic', 'sympathetic', 'neutral', 'sympathetic'], ['neutral', 'sympathetic', 'neutral', 'neutral', 'sympathetic']], 'percent_dominant_sentiments': [[0.423728813559322, 0.42857142857142855, 0.42857142857142855, 0.391304347826087, 0.42424242424242425], [0.4142857142857143, 0.4358974358974359, 0.38235294117647056, 0.39473684210526316, 0.4805194805194805], [0.4444444444444444, 0.527027027027027, 0.41333333333333333, 0.4177215189873418, 0.5375], [0.37142857142857144, 0.4153846153846154, 0.463768115942029, 0.44776119402985076, 0.4915254237288136], [0.5263157894736842, 0.4166666666666667, 0.38271604938271603, 0.3492063492063492, 0.4225352112676056]], 'num_targ_conc': 43, 'num_indisc_conc': 54, 'num_targ_repr': 18, 'num_indisc_repr': 17}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 200}, 'final_pop': 853, 'total_num_attacks': 527, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.723404255319149, 0.6962962962962963, 0.7516339869281046], [0.6891891891891891, 0.6, 0.5639097744360902]], 'num_targ_conc': 24, 'num_indisc_conc': 43, 'num_targ_repr': 8, 'num_indisc_repr': 8}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'CONC', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 200, 'steps': 300}, 'final_pop': 1356, 'total_num_attacks': 990, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.8728070175438597, 0.8927038626609443, 0.8924302788844621], [0.8511627906976744, 0.8201754385964912, 0.8794642857142857]], 'num_targ_conc': 42, 'num_indisc_conc': 63, 'num_targ_repr': 21, 'num_indisc_repr': 11}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 404, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 6, 'num_indisc_conc': 13, 'num_targ_repr': 32, 'num_indisc_repr': 46}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 1, 'total_num_attacks': 887, 'dominant_sentiments': [['NONE', 'anti-violence', 'sympathetic', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 12, 'num_indisc_conc': 9, 'num_targ_repr': 32, 'num_indisc_repr': 41}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'low', 'discontent': 'mid', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 375, 'dominant_sentiments': [['INDISC-REPR', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 10, 'num_indisc_conc': 13, 'num_targ_repr': 34, 'num_indisc_repr': 37}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 200}, 'final_pop': 24, 'total_num_attacks': 641, 'dominant_sentiments': [['neutral', 'neutral', 'sympathetic'], ['neutral', 'sympathetic', 'anti-violence']], 'percent_dominant_sentiments': [[0.5714285714285714, 0.6666666666666666, 1.0], [0.75, 1.0, 0.4]], 'num_targ_conc': 7, 'num_indisc_conc': 13, 'num_targ_repr': 37, 'num_indisc_repr': 31}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 300}, 'final_pop': 195, 'total_num_attacks': 673, 'dominant_sentiments': [['neutral', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.3548387096774194, 0.37142857142857144, 0.6], [0.8666666666666667, 0.6129032258064516, 0.5]], 'num_targ_conc': 12, 'num_indisc_conc': 21, 'num_targ_repr': 53, 'num_indisc_repr': 47}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 200, 'steps': 500}, 'final_pop': 190, 'total_num_attacks': 537, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.7941176470588235, 1.0, 0.8181818181818182], [0.7941176470588235, 1.0, 0.7575757575757576]], 'num_targ_conc': 26, 'num_indisc_conc': 29, 'num_targ_repr': 86, 'num_indisc_repr': 94}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 300, 'steps': 500}, 'final_pop': 173, 'total_num_attacks': 1102, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.65, 0.8181818181818182, 0.4444444444444444], [0.7619047619047619, 0.7, 0.8181818181818182], [0.7777777777777778, 0.631578947368421, 0.6086956521739131]], 'num_targ_conc': 21, 'num_indisc_conc': 26, 'num_targ_repr': 80, 'num_indisc_repr': 96}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 600, 'steps': 500}, 'final_pop': 0, 'total_num_attacks': 1007, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 30, 'num_indisc_conc': 24, 'num_targ_repr': 97, 'num_indisc_repr': 73}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 200}, 'final_pop': 309, 'total_num_attacks': 1769, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.75, 0.875, 0.7142857142857143, 0.8571428571428571, 0.8947368421052632], [1.0, 1.0, 1.0, 0.9230769230769231, 0.875], [0.8181818181818182, 0.8947368421052632, 0.8636363636363636, 0.8571428571428571, 0.75], [0.9166666666666666, 0.8666666666666667, 0.8461538461538461, 0.6666666666666666, 0.625], [0.9090909090909091, 0.8421052631578947, 1.0, 0.9375, 1.0]], 'num_targ_conc': 7, 'num_indisc_conc': 5, 'num_targ_repr': 29, 'num_indisc_repr': 39}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'high', 'starting_population': 800, 'steps': 500}, 'final_pop': 506, 'total_num_attacks': 4828, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'neutral', 'neutral'], ['anti-violence', 'anti-violence', 'anti-violence', 'neutral', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'neutral']], 'percent_dominant_sentiments': [[0.5, 0.5, 0.6153846153846154, 0.5, 0.5], [0.75, 0.5, 0.5, 0.47368421052631576, 0.5416666666666666], [0.6538461538461539, 0.5833333333333334, 0.56, 0.6666666666666666, 0.7222222222222222], [0.4375, 0.75, 0.5, 0.6, 0.6111111111111112], [0.45454545454545453, 0.4444444444444444, 0.8, 0.4, 0.5333333333333333]], 'num_targ_conc': 23, 'num_indisc_conc': 19, 'num_targ_repr': 94, 'num_indisc_repr': 82}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 200, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 444, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 5, 'num_indisc_conc': 15, 'num_targ_repr': 32, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 200}, 'final_pop': 538, 'total_num_attacks': 1419, 'dominant_sentiments': [['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'sympathetic', 'anti-violence', 'anti-violence'], ['neutral', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic'], ['anti-violence', 'sympathetic', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[0.6551724137931034, 0.68, 0.5263157894736842, 0.6896551724137931, 0.6153846153846154], [0.6, 0.4583333333333333, 0.42857142857142855, 0.7083333333333334, 0.42857142857142855], [0.4838709677419355, 0.4, 0.5357142857142857, 0.5, 0.5185185185185185], [0.7916666666666666, 0.4166666666666667, 0.5, 0.41379310344827586, 0.46875]], 'num_targ_conc': 6, 'num_indisc_conc': 6, 'num_targ_repr': 45, 'num_indisc_repr': 29}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 300}, 'final_pop': 0, 'total_num_attacks': 2460, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 12, 'num_indisc_conc': 11, 'num_targ_repr': 55, 'num_indisc_repr': 43}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'mid', 'starting_population': 600, 'steps': 500}, 'final_pop': 68, 'total_num_attacks': 3598, 'dominant_sentiments': [['sympathetic', 'anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'neutral', 'anti-violence', 'anti-violence'], ['neutral', 'neutral', 'neutral', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence', 'sympathetic']], 'percent_dominant_sentiments': [[0.5, 0.3333333333333333, 0.5, 1.0, 0.6], [0.5, 0.6, 0.42857142857142855, 0.4, 0.4], [0.5, 1.0, 0.6, 1.0, 0.75], [0.8, 1.0, 0.5, 0.5, 1.0]], 'num_targ_conc': 25, 'num_indisc_conc': 12, 'num_targ_repr': 111, 'num_indisc_repr': 75}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 300, 'steps': 700}, 'final_pop': 0, 'total_num_attacks': 621, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 37, 'num_indisc_conc': 38, 'num_targ_repr': 111, 'num_indisc_repr': 134}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 400, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 627, 'dominant_sentiments': [['NONE', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]], 'num_targ_conc': 6, 'num_indisc_conc': 9, 'num_targ_repr': 34, 'num_indisc_repr': 38}, {'params': {'prob_violence': 0.0001, 'govt_policy': 'REPR', 'reactive_lvl': 'none', 'discontent': 'low', 'starting_population': 800, 'steps': 200}, 'final_pop': 758, 'total_num_attacks': 2472, 'dominant_sentiments': [['neutral', 'neutral', 'neutral', 'neutral', 'anti-violence'], ['anti-violence', 'neutral', 'neutral', 'anti-violence', 'anti-violence'], ['anti-violence', 'neutral', 'neutral', 'sympathetic', 'neutral'], ['neutral', 'neutral', 'neutral', 'neutral', 'sympathetic'], ['neutral', 'anti-violence', 'sympathetic', 'neutral', 'anti-violence']], 'percent_dominant_sentiments': [[0.5172413793103449, 0.5357142857142857, 0.3548387096774194, 0.5483870967741935, 0.4411764705882353], [0.45161290322580644, 0.3939393939393939, 0.5588235294117647, 0.37037037037037035, 0.41379310344827586], [0.6206896551724138, 0.4, 0.5882352941176471, 0.41935483870967744, 0.4827586206896552], [0.45161290322580644, 0.45161290322580644, 0.4230769230769231, 0.43333333333333335, 0.44], [0.5, 0.5806451612903226, 0.4375, 0.38235294117647056, 0.46875]], 'num_targ_conc': 6, 'num_indisc_conc': 10, 'num_targ_repr': 30, 'num_indisc_repr': 28}, {'params': {'prob_violence': 0.0005, 'govt_policy': 'NONE', 'reactive_lvl': 'high', 'discontent': 'high', 'starting_population': 300, 'steps': 200}, 'final_pop': 0, 'total_num_attacks': 332, 'dominant_sentiments': [['TARG-REPR', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence'], ['anti-violence', 'anti-violence', 'anti-violence']], 'percent_dominant_sentiments': [[1.0, 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]], 'num_targ_conc': 17, 'num_indisc_conc': 20, 'num_targ_repr': 16, 'num_indisc_repr': 30}]
overall runtime for 492 parameter combinations on 18 nodes        was 51.25027346611023
